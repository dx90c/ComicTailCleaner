# ======================================================================
# Ê™îÊ°àÂêçÁ®±Ôºöcore_engine.py
# Ê®°ÁµÑÁõÆÁöÑÔºöÂåÖÂê´Ê†∏ÂøÉÁöÑÊØîÂ∞çÂºïÊìé„ÄÅÊ™îÊ°àÊéÉÊèèËàáÂø´ÂèñÁÆ°ÁêÜÈÇèËºØ
# ÁâàÊú¨Ôºö2.2.0 (ÂºïÂÖ•Âª£ÂëäÊØîÂ∞ç Epoch Ê©üÂà∂‰ª•ÂØ¶ÁèæÂ¢ûÈáèÊØîÂ∞ç)
# ======================================================================

import os
import re
import json
import time
import datetime
import sys
import platform
from collections import deque, defaultdict
from multiprocessing import Pool, cpu_count, Event, set_start_method
from queue import Queue
from typing import Union, Tuple, Dict, List, Set, Optional, Generator, Any

# --- Á¨¨‰∏âÊñπÂ∫´ ---
try:
    import imagehash
except ImportError:
    imagehash = None
try:
    import send2trash
except ImportError:
    send2trash = None
try:
    import cv2
    import numpy as np
except ImportError:
    cv2 = None
    np = None

# --- Êú¨Âú∞Ê®°ÁµÑ ---
try:
    import archive_handler
    ARCHIVE_SUPPORT_ENABLED = True
except ImportError:
    archive_handler = None
    ARCHIVE_SUPPORT_ENABLED = False

import config
import utils
from utils import (log_info, log_error, log_performance, _is_virtual_path,
                   _parse_virtual_path, _open_image_from_any_path,
                   _get_file_stat, sim_from_hamming, hamming_from_sim,
                   _avg_hsv, _color_gate, CACHE_LOCK, _norm_key)

try:
    from utils import log_warning
except ImportError:
    def log_warning(msg: str): print(f"[WARN] {msg}")

from processors.scanner import ScannedImageCacheManager, FolderStateCacheManager
try:
    from processors.qr_engine import (_pool_worker_detect_qr_code,
                                     _pool_worker_process_image_full,
                                     _pool_worker_process_image_phash_only)
    QR_ENGINE_ENABLED = True
except ImportError:
    utils.log_warning("[Ë≠¶Âëä] ÁÑ°Ê≥ïÂæû processors.qr_engine Â∞éÂÖ• QR workerÔºåQR Áõ∏ÈóúÂäüËÉΩÂ∞á‰∏çÂèØÁî®„ÄÇ")
    _pool_worker_detect_qr_code = None
    _pool_worker_process_image_full = None
    _pool_worker_process_image_phash_only = None
    QR_ENGINE_ENABLED = False

# ======================================================================
# Section: ÂÖ®Â±ÄÂ∏∏ÈáèËàáËºîÂä©ÂáΩÂºè
# ======================================================================
__version__ = "2.2.0"  # ÂñÆ‰∏ÄÁúüÁõ∏‰æÜÊ∫ê
HASH_BITS = 64
PHASH_FAST_THRESH   = 0.80
PHASH_STRICT_SKIP   = 0.93
WHASH_TIER_1        = 0.90
WHASH_TIER_2        = 0.92
WHASH_TIER_3        = 0.95
WHASH_TIER_4        = 0.98
AD_GROUPING_THRESHOLD = 0.95
LSH_BANDS = 4

def _natural_sort_key(s: str) -> list:
    return [int(text) if text.isdigit() else text.lower() for text in re.split(r'([0-9]+)', s)]

def _folder_time(st: os.stat_result, mode: str) -> float:
    if mode == 'ctime':  return st.st_ctime
    if mode == 'hybrid': return max(st.st_mtime, st.st_ctime)
    return st.st_mtime

def _iter_scandir_recursively(root_path: str, excluded_paths: set, excluded_names: set, control_events: Optional[dict]) -> Generator[os.DirEntry, None, None]:
    queue = deque([root_path])
    while queue:
        if control_events and control_events.get('cancel') and control_events['cancel'].is_set():
            return
        
        current_dir = queue.popleft()
        try:
            with os.scandir(current_dir) as it:
                for entry in it:
                    norm_path = _norm_key(entry.path)
                    base_name = os.path.basename(norm_path)
                    
                    if any(norm_path == ex or norm_path.startswith(ex + os.sep) for ex in excluded_paths) or base_name in excluded_names:
                        continue

                    if entry.is_dir(follow_symlinks=False):
                        queue.append(entry.path)
                    elif entry.is_file():
                        yield entry
        except OSError:
            continue

def _iter_scandir_time_pruned(root_path: str, excluded_paths: set, excluded_names: set,
                              control_events: Optional[dict], max_depth: int,
                              start: Optional[datetime.datetime], end: Optional[datetime.datetime], time_mode: str) -> Generator[os.DirEntry, None, None]:
    root_norm = _norm_key(root_path)
    base_depth = root_norm.count(os.sep)
    queue = deque([root_path])
    while queue:
        if control_events and control_events.get('cancel') and control_events['cancel'].is_set():
            return
        cur = queue.popleft()
        norm_cur = _norm_key(cur)
        if any(norm_cur == ex or norm_cur.startswith(ex + os.sep) for ex in excluded_paths) \
           or os.path.basename(norm_cur).lower() in excluded_names:
            continue
        cur_depth = norm_cur.count(os.sep) - base_depth
        try:
            with os.scandir(cur) as it:
                for entry in it:
                    if entry.is_file():
                        yield entry
                    elif entry.is_dir(follow_symlinks=False):
                        try:
                            st = entry.stat(follow_symlinks=False)
                            ts = _folder_time(st, time_mode)
                            if start and datetime.datetime.fromtimestamp(ts) < start:
                                continue
                            if end and datetime.datetime.fromtimestamp(ts) > end:
                                continue
                        except OSError:
                            continue
                        if cur_depth < max_depth:
                            queue.append(entry.path)
        except OSError:
            continue

# ======================================================================
# Section: È´òÊïàÊ™îÊ°àÂàóËàâ
# ======================================================================

def _scan_newest_first_recursive(path: str, time_filter: dict, excluded_paths: set, excluded_names: set, control_events: Optional[dict], stats: Dict[str, int], time_mode: str) -> Generator[str, None, None]:
    if control_events and control_events.get('cancel') and control_events['cancel'].is_set():
        return

    norm_path = _norm_key(path)
    base_name = os.path.basename(norm_path).lower()
    if any(norm_path == ex or norm_path.startswith(ex + os.sep) for ex in excluded_paths) or base_name in excluded_names:
        return

    try:
        stats['visited_dirs'] += 1
        st = os.stat(path)
        cur_ts = _folder_time(st, time_mode)
        mtime_dt = datetime.datetime.fromtimestamp(cur_ts)
        start = time_filter.get('start')
        end   = time_filter.get('end')

        if start and mtime_dt < start:
            stats['pruned_by_start'] += 1
            return

        in_range = (not start or mtime_dt >= start) and (not end or mtime_dt <= end)
        if in_range:
            yield path
        elif end and mtime_dt > end:
            stats['skipped_by_end'] += 1

        subdirs = []
        with os.scandir(path) as it:
            for entry in it:
                if control_events and control_events.get('cancel') and control_events['cancel'].is_set(): return
                if entry.is_dir(follow_symlinks=False):
                    try:
                        st_sub = entry.stat(follow_symlinks=False)
                        subdirs.append((entry.path, _folder_time(st_sub, time_mode)))
                    except OSError:
                        continue
        
        subdirs.sort(key=lambda x: x[1], reverse=True)

        processed_count = 0
        for subdir_path, mt in subdirs:
            if control_events and control_events.get('cancel') and control_events['cancel'].is_set(): return
            if start and datetime.datetime.fromtimestamp(mt) < start:
                stats['pruned_by_start'] += (len(subdirs) - processed_count)
                break
            processed_count += 1
            yield from _scan_newest_first_recursive(subdir_path, time_filter, excluded_paths, excluded_names, control_events, stats, time_mode)
    except OSError:
        return

def _unified_scan_traversal(root_folder: str, excluded_paths: set, excluded_names: set, time_filter: dict, folder_cache: 'FolderStateCacheManager', progress_queue: Optional[Queue], control_events: Optional[dict], use_pruning: bool, time_mode: str) -> Tuple[Dict[str, Any], Set[str], Set[str]]:
    log_info(f"ÂïüÂãï v{__version__} Áµ±‰∏ÄÊéÉÊèèÂºïÊìé...")
    
    if not use_pruning or not time_filter.get('enabled') or not time_filter.get('start'):
        log_info("‰ΩøÁî®Ê®ôÊ∫ñ BFS ÊéÉÊèè (Êú™ÂïüÁî®Ââ™ÊûùÊàñÊôÇÈñìÁØ©ÈÅ∏)„ÄÇ")
        live_folders, changed_or_new_folders = {}, set()
        queue = deque([root_folder])
        scanned_count = 0
        cached_states = folder_cache.cache.copy()
        
        while queue:
            if control_events and control_events.get('cancel') and control_events['cancel'].is_set():
                return {}, set(), set()
            current_dir = queue.popleft()
            norm_current_dir = _norm_key(current_dir)
            if any(norm_current_dir == ex or norm_current_dir.startswith(ex + os.sep) for ex in excluded_paths) or os.path.basename(norm_current_dir).lower() in excluded_names:
                continue
            try:
                stat_info = os.stat(current_dir)
                cached_states.pop(norm_current_dir, None)
                scanned_count += 1
                if scanned_count % 100 == 0:
                    time.sleep(0.001)
                    if progress_queue: progress_queue.put({'type': 'text', 'text': f"üìÅ Ê≠£Âú®Ê™¢Êü•Ë≥áÊñôÂ§æÁµêÊßã... ({scanned_count})"})

                live_folders[norm_current_dir] = {'mtime': stat_info.st_mtime, 'ctime': stat_info.st_ctime}
                cached_entry = folder_cache.get_folder_state(norm_current_dir)
                if not cached_entry or abs(_folder_time(stat_info, time_mode) - cached_entry.get('mtime', 0)) > 1e-6:
                    changed_or_new_folders.add(norm_current_dir)

                with os.scandir(current_dir) as it:
                    for entry in it:
                        if entry.is_dir(follow_symlinks=False):
                            queue.append(entry.path)
            except OSError: continue
        
        ghost_folders = set(cached_states.keys())
        log_info(f"BFS ÊéÉÊèèÂÆåÊàê„ÄÇÂç≥ÊôÇË≥áÊñôÂ§æ: {len(live_folders)}, Êñ∞/ËÆäÊõ¥: {len(changed_or_new_folders)}, ÂπΩÈùàË≥áÊñôÂ§æ: {len(ghost_folders)}")
        return live_folders, changed_or_new_folders, ghost_folders

    log_info("ÂïüÁî®ÊôÇÈñìÁØ©ÈÅ∏Ôºå‰ΩøÁî®Êô∫ÊÖßÂûãÈÅûËø¥Ââ™Êûù (DFS) ÊéÉÊèè...")
    stats = defaultdict(int)
    all_scanned_paths = list(_scan_newest_first_recursive(root_folder, time_filter, excluded_paths, excluded_names, control_events, stats, time_mode))
    
    live_folders, changed_or_new_folders = {}, set()
    cached_states = folder_cache.cache.copy()

    for path in all_scanned_paths:
        norm_path = _norm_key(path)
        cached_states.pop(norm_path, None)
        try:
            stat_info = os.stat(path)
            live_folders[norm_path] = {'mtime': stat_info.st_mtime, 'ctime': stat_info.st_ctime}
            cached_entry = folder_cache.get_folder_state(norm_path)
            if not cached_entry or abs(_folder_time(stat_info, time_mode) - cached_entry.get('mtime', 0)) > 1e-6:
                changed_or_new_folders.add(norm_path)
        except OSError:
            continue
            
    ghost_folders = set(cached_states.keys())
    log_info(f"DFS ÊéÉÊèèÂÆåÊàê„ÄÇË®™Âïè: {stats['visited_dirs']}, Ëµ∑ÂßãÊó•Ââ™Êûù: {stats['pruned_by_start']}, ÁµêÊùüÊó•Ë∑≥ÈÅé: {stats['skipped_by_end']}")
    log_info(f"Á¨¶ÂêàÊôÇÈñìÁöÑË≥áÊñôÂ§æ: {len(live_folders)}, Êñ∞/ËÆäÊõ¥: {len(changed_or_new_folders)}, ÂπΩÈùàË≥áÊñôÂ§æ: {len(ghost_folders)}")
    return live_folders, changed_or_new_folders, ghost_folders
    
def get_files_to_process(config_dict: Dict, image_cache_manager: ScannedImageCacheManager, progress_queue: Optional[Queue] = None, control_events: Optional[Dict] = None) -> Tuple[List[str], Dict[str, int]]:
    root_folder = config_dict['root_scan_folder']
    if not os.path.isdir(root_folder): return [], {}
    
    enable_archive_scan = config_dict.get('enable_archive_scan', False) and ARCHIVE_SUPPORT_ENABLED
    
    fmts = []
    if enable_archive_scan:
        for e in archive_handler.get_supported_formats():
            e = e.lower().strip()
            if not e.startswith('.'):
                e = '.' + e
            fmts.append(e)
    supported_archive_exts = tuple(fmts)

    folder_cache = FolderStateCacheManager(root_folder)
    
    excluded_folders_config = config_dict.get('excluded_folders', [])
    excluded_paths = {_norm_key(p) for p in excluded_folders_config if os.path.sep in p or (os.path.altsep and os.path.altsep in p)}
    excluded_names = {name.lower() for name in excluded_folders_config if (os.path.sep not in name) and (not os.path.altsep or os.path.altsep not in name)}

    time_filter = {'enabled': config_dict.get('enable_time_filter', False)}
    if time_filter['enabled']:
        try:
            start_str, end_str = config_dict.get('start_date_filter'), config_dict.get('end_date_filter')
            time_filter['start'] = datetime.datetime.strptime(start_str, "%Y-%m-%d") if start_str else None
            time_filter['end'] = datetime.datetime.strptime(end_str, "%Y-%m-%d").replace(hour=23, minute=59, second=59) if end_str else None
        except (ValueError, TypeError):
            log_error("ÊôÇÈñìÁØ©ÈÅ∏Êó•ÊúüÊ†ºÂºèÈåØË™§ÔºåÂ∞áË¢´ÂøΩÁï•„ÄÇ"); time_filter['enabled'] = False

    use_pruning = config_dict.get('enable_newest_first_pruning', True)
    time_mode = str(config_dict.get('folder_time_mode', 'mtime'))
    live_folders, folders_to_scan_content, ghost_folders = _unified_scan_traversal(root_folder, excluded_paths, excluded_names, time_filter, folder_cache, progress_queue, control_events, use_pruning, time_mode)

    root_norm = _norm_key(config_dict['root_scan_folder'])
    if root_norm in folders_to_scan_content:
        log_warning("[‰øùË≠∑] Ê†πË≥áÊñôÂ§æË¢´Ê®ôË®òÁÇ∫„ÄéËÆäÊõ¥„Äè‚Äî Â∞áÊîπÁî®‰øùÂ∫ïÊ®°ÂºèÔºàÂÉÖË£úÂø´ÂèñÁº∫Âè£Ôºå‰∏çÂÖ®Èù¢ÈÅûËø¥Ôºâ„ÄÇ")
        folders_to_scan_content.discard(root_norm)

    def _prune_to_leaf_changed(changed_set: set[str]) -> set[str]:
        changed = sorted(changed_set)
        if not changed: return set()
        keep = set(changed)
        for i, p in enumerate(changed):
            prefix = p + os.sep
            for j in range(i + 1, len(changed)):
                q = changed[j]
                if q.startswith(prefix):
                    keep.discard(p)
                    break
        return keep

    orig_cnt = len(folders_to_scan_content)
    folders_to_scan_content = _prune_to_leaf_changed(folders_to_scan_content)
    if len(folders_to_scan_content) < orig_cnt:
        log_info(f"[ËÆäÊõ¥ÈõÜÁ∏ÆÊ∏õ] Áî± {orig_cnt} Â§æÁ∏ÆËá≥ {len(folders_to_scan_content)} ÂÄãÊúÄÊ∑±ËÆäÊõ¥Â§æÔºåÈÅøÂÖçÊï¥Ê£µÊ®πÈáçÊéÉ„ÄÇ")
    
    if control_events and control_events.get('cancel') and control_events['cancel'].is_set(): return [], {}

    use_time_window = bool(time_filter.get('enabled') and (time_filter.get('start') or time_filter.get('end')))
    preserve = bool(config_dict.get('preserve_cache_across_time_windows', True))
    strict_img_prune = bool(config_dict.get('prune_image_cache_on_missing_folder', False))

    if ghost_folders:
        if use_time_window and preserve:
            truly_missing = [f for f in ghost_folders if not os.path.exists(f)]
            if truly_missing:
                log_info(f"Ê≠£Âú®ÂæûÁãÄÊÖãÂø´Âèñ‰∏≠ÁßªÈô§ {len(truly_missing)} ÂÄãÂ∑≤‰∏çÂ≠òÂú®ÁöÑË≥áÊñôÂ§æ...")
                folder_cache.remove_folders(truly_missing)
                if strict_img_prune:
                    log_info(f"Ê≠£Âú®ÂêåÊ≠•ÁßªÈô§Â∞çÊáâÁöÑÂúñÁâáÂø´Âèñ...")
                    for folder in truly_missing:
                        image_cache_manager.remove_entries_from_folder(folder)
        else:
            log_info(f"Ê≠£Âú®Ê∏ÖÁêÜ {len(ghost_folders)} ÂÄãÂπΩÈùàË≥áÊñôÂ§æÁöÑÂø´Âèñ...")
            folder_cache.remove_folders(list(ghost_folders))
            for folder in ghost_folders:
                image_cache_manager.remove_entries_from_folder(folder)

    unchanged_folders = set(live_folders.keys()) - folders_to_scan_content
    
    folders_in_cache = set()
    for p in image_cache_manager.cache.keys():
        try:
            if _is_virtual_path(p):
                arch_path, _ = _parse_virtual_path(p)
                if arch_path: folders_in_cache.add(_norm_key(os.path.dirname(arch_path)))
            else:
                folders_in_cache.add(_norm_key(os.path.dirname(p)))
        except Exception: continue
    
    augmented_cache_folders = set(folders_in_cache)
    for f in list(folders_in_cache):
        cur = f
        while True:
            parent = _norm_key(os.path.dirname(cur))
            if not parent or parent == cur: break
            if parent in augmented_cache_folders: break
            augmented_cache_folders.add(parent)
            cur = parent

    folders_needing_scan_due_to_empty_cache = unchanged_folders - augmented_cache_folders

    if root_norm in folders_needing_scan_due_to_empty_cache:
        folders_needing_scan_due_to_empty_cache.discard(root_norm)
        log_warning("[‰øùË≠∑] Ê†πÂ§æÁº∫Âø´Âèñ‰ΩÜÂ∑≤Ë∑≥ÈÅé‰øùÂ∫ïË£úÊéÉÔºõÂ¶ÇÈúÄÊéÉÊèèË´ãÊîπÈÅ∏Â≠êË≥áÊñôÂ§æÊàñÂèñÊ∂àÊ†πÂ§æ‰øùË≠∑„ÄÇ")

    if folders_needing_scan_due_to_empty_cache:
        def _prune_to_leaf_only(cands: Set[str]) -> Set[str]:
            items = sorted(cands)
            keep = set(items)
            for i, p in enumerate(items):
                pref = p + os.sep
                for j in range(i + 1, len(items)):
                    if items[j].startswith(pref):
                        keep.discard(p)
                        break
            return keep

        pruned = _prune_to_leaf_only(folders_needing_scan_due_to_empty_cache)
        if pruned != folders_needing_scan_due_to_empty_cache:
            log_info(f"[‰øùÂ∫ïË£ÅÂâ™] Âæû {len(folders_needing_scan_due_to_empty_cache)} Â§æË£ÅÊàê {len(pruned)} ÂÄãËëâÁØÄÈªûÂ§æ‰ª•ÈÅøÂÖçÊï¥Ê®πË£úÊéÉ„ÄÇ")
        folders_needing_scan_due_to_empty_cache = pruned

        log_info(f"[‰øùÂ∫ï] {len(folders_needing_scan_due_to_empty_cache)} ÂÄãÊú™ËÆäÊõ¥Ë≥áÊñôÂ§æÂõ†Âú®ÂúñÁâáÂø´Âèñ‰∏≠ÁÑ°Ë®òÈåÑÔºåÂ∑≤Âä†ÂÖ•ÊéÉÊèè„ÄÇ")
        folders_to_scan_content.update(folders_needing_scan_due_to_empty_cache)
        unchanged_folders -= folders_needing_scan_due_to_empty_cache

    image_exts = ('.jpg', '.jpeg', '.png', '.gif', '.bmp', '.webp', '.tiff')
    count, enable_limit = config_dict.get('extract_count', 8), config_dict.get('enable_extract_count_limit', True)
    
    qr_mode = config_dict.get('comparison_mode') == 'qr_detection'
    if qr_mode and enable_limit:
        count = config_dict.get('qr_pages_per_archive', 10)
    qr_global_cap = config_dict.get('qr_global_cap', 20000)

    scanned_files = []
    
    changed_container_cap = int(config_dict.get('changed_container_cap', 0) or 0)
    depth_limit = int(config_dict.get('changed_container_depth_limit', 1))
    start, end = time_filter.get('start'), time_filter.get('end')

    def _container_mtime(p: str) -> float:
        try: return os.path.getmtime(p)
        except OSError: return 0.0

    for folder in sorted(list(folders_to_scan_content)):
        if control_events and control_events.get('cancel') and control_events['cancel'].is_set(): break
        
        temp_files_in_container = defaultdict(list)
        
        for entry in _iter_scandir_time_pruned(folder, excluded_paths, excluded_names, control_events, depth_limit, start, end, time_mode):
            f_lower = entry.name.lower()
            if enable_archive_scan and f_lower.endswith(supported_archive_exts):
                temp_files_in_container[entry.path] = []
            elif f_lower.endswith(image_exts):
                temp_files_in_container[os.path.dirname(entry.path)].append(_norm_key(entry.path))
        
        if changed_container_cap > 0 and len(temp_files_in_container) > changed_container_cap:
            keep = sorted(temp_files_in_container.keys(), key=_container_mtime, reverse=True)[:changed_container_cap]
            dropped = len(temp_files_in_container) - len(keep)
            temp_files_in_container = {k: temp_files_in_container[k] for k in keep}
            log_info(f"[ËÆäÊõ¥Â§æÂÆπÂô®‰∏äÈôê] {folder} ÂÉÖ‰øùÁïô {len(keep)} ÂÄãËøëÊúüÂÆπÂô®ÔºåÊç®Ê£Ñ {dropped} ÂÄã")

        for container_path, files in temp_files_in_container.items():
            ext = os.path.splitext(container_path)[1].lower()
            if ext in supported_archive_exts:
                try:
                    all_vpaths = []
                    for arc_entry in archive_handler.iter_archive_images(container_path):
                        vpath = f"{config.VPATH_PREFIX}{arc_entry.archive_path}{config.VPATH_SEPARATOR}{arc_entry.inner_path}"
                        all_vpaths.append(vpath)
                    all_vpaths.sort(key=_natural_sort_key)
                    files.extend(all_vpaths)
                except Exception as e:
                    log_error(f"ËÆÄÂèñÂ£ìÁ∏ÆÊ™îÂ§±Êïó: {container_path}: {e}", True)
                    continue
            files.sort(key=_natural_sort_key)
            if enable_limit:
                scanned_files.extend(files[-count:])
            else:
                scanned_files.extend(files)

        norm_folder = _norm_key(folder)
        if norm_folder in live_folders: 
            folder_cache.update_folder_state(norm_folder, live_folders[norm_folder]['mtime'], live_folders[norm_folder]['ctime'])

    if control_events and control_events.get('cancel') and control_events['cancel'].is_set(): return [], {}
    
    cached_files = []
    if unchanged_folders:
        by_container = defaultdict(list)
        for p, meta in image_cache_manager.cache.items():
            try:
                container_key = ""
                parent_dir = ""
                if _is_virtual_path(p):
                    archive_path, _ = _parse_virtual_path(p)
                    if archive_path:
                        container_key = archive_path
                        parent_dir = _norm_key(os.path.dirname(archive_path))
                else:
                    if _get_file_stat(p)[2] is None: continue
                    parent_dir = _norm_key(os.path.dirname(p))
                    container_key = parent_dir
                
                if parent_dir in unchanged_folders:
                    by_container[container_key].append(p)
            except Exception:
                continue
        
        for container, lst in by_container.items():
            lst.sort(key=_natural_sort_key)
            take = lst[-count:] if enable_limit else lst
            cached_files.extend(take)
            
    final_file_list = scanned_files + cached_files
    folder_cache.save_cache()
    unique_files = sorted(list(set(final_file_list)))
    
    def _path_mtime_for_cap(p: str) -> float:
        try:
            if _is_virtual_path(p):
                arch_path, _ = _parse_virtual_path(p)
                return os.path.getmtime(arch_path) if arch_path and os.path.exists(arch_path) else 0.0
            else:
                return os.path.getmtime(p) if os.path.exists(p) else 0.0
        except Exception: return 0.0

    mode = str(config_dict.get('comparison_mode', '')).lower()
    global_cap = int(config_dict.get('global_extract_cap', 0) or 0)

    if mode != 'qr_detection' and global_cap > 0 and len(unique_files) > global_cap:
        unique_files.sort(key=_path_mtime_for_cap, reverse=True)
        kept = unique_files[:global_cap]
        log_warning(f"[ÂÖ®Âüü‰∏äÈôê] ÊèêÂèñ {len(unique_files)} ‚Üí {global_cap}Ôºà‰æù mtime ÁØ©ÈÅ∏ÊúÄÊñ∞Ôºâ")
        unique_files = kept
    elif qr_mode and not enable_limit and qr_global_cap > 0 and len(unique_files) > qr_global_cap:
        log_error(f"[Èò≤ÁàÜÈáè] ÊèêÂèñÁ∏ΩÊï∏ {len(unique_files)} Ë∂ÖÈÅéÂÖ®Âüü‰∏äÈôê {qr_global_cap}ÔºåÂ∞áÂè™ËôïÁêÜÊúÄÊñ∞ {qr_global_cap} Á≠Ü„ÄÇ")
        unique_files.sort(key=_path_mtime_for_cap, reverse=True)
        unique_files = unique_files[:qr_global_cap]
        
    log_info(f"Ê™îÊ°àÊèêÂèñÂÆåÊàê„ÄÇÂæû {len(folders_to_scan_content)} ÂÄãÊñ∞/ËÆäÊõ¥Â§æÊéÉÊèè {len(scanned_files)} Á≠Ü, Âæû {len(unchanged_folders)} ÂÄãÊú™ËÆäÊõ¥Â§æÊÅ¢Âæ© {len(cached_files)} Á≠Ü„ÄÇÁ∏ΩË®à: {len(unique_files)}")
    return unique_files, {}

# ======================================================================
# Section: Ê†∏ÂøÉÊØîÂ∞çÂºïÊìé
# ======================================================================

class ImageComparisonEngine:
    def __init__(self, config_dict: dict, progress_queue: Union[Queue, None] = None, control_events: Union[dict, None] = None):
        self.config = config_dict; self.progress_queue = progress_queue; self.control_events = control_events
        self.system_qr_scan_capability = QR_ENGINE_ENABLED
        self.pool = None; self.file_data = {}; self.tasks_to_process = []
        self.total_task_count = 0; self.completed_task_count = 0; self.failed_tasks = []
        self.vpath_size_map = {}
        log_performance("[ÂàùÂßãÂåñ] ÊéÉÊèèÂºïÊìéÂØ¶‰æã")
        
    def _check_control(self) -> str:
        if self.control_events:
            if self.control_events.get('cancel') and self.control_events['cancel'].is_set(): return 'cancel'
            if self.control_events.get('pause') and self.control_events['pause'].is_set(): return 'pause'
        return 'continue'
        
    def _update_progress(self, p_type: str = 'text', value: Union[int, None] = None, text: Union[str, None] = None) -> None:
        if self.progress_queue: self.progress_queue.put({'type': p_type, 'value': value, 'text': text})
        
    def _cleanup_pool(self):
        if self.pool:
            log_info("Ê≠£Âú®ÁµÇÁµêÁèæÊúâÈÄ≤Á®ãÊ±†...");
            if self.progress_queue: self.progress_queue.put({'type': 'status_update', 'text': "Ê≠£Âú®ÁµÇÊ≠¢ËÉåÊôØ‰ªªÂãô..."})
            self.pool.terminate(); self.pool.join()
            log_info("ÈÄ≤Á®ãÊ±†Â∑≤ÊàêÂäüÁµÇÁµê„ÄÇ"); self.pool = None
            
    def find_duplicates(self) -> Union[tuple[list, dict, list], None]:
        try:
            self._update_progress(text="‰ªªÂãôÈñãÂßã..."); log_performance("[ÈñãÂßã] ÊéÉÊèè‰ªªÂãô")
            root_scan_folder = self.config.get('root_scan_folder')
            ad_folder_path = self.config.get('ad_folder_path')
            scan_cache_manager = ScannedImageCacheManager(root_scan_folder, ad_folder_path, self.config.get('comparison_mode'))
            
            try:
                mode = str(self.config.get("comparison_mode", "mutual_comparison")).lower()
                mode_map = { "ad_comparison": "Âª£ÂëäÊØîÂ∞ç", "mutual_comparison": "‰∫íÁõ∏ÊØîÂ∞ç", "qr_detection": "QR Code Ê™¢Ê∏¨" }
                mode_str = mode_map.get(mode, "Êú™Áü•")
                log_info("=" * 50)
                log_info(f"[ÂºïÊìéÁâàÊú¨] Ê†∏ÂøÉÂºïÊìé v{__version__}")
                log_info(f"[Ê®°ÂºèÊ™¢Êü•] Áï∂ÂâçÊ®°Âºè: {mode_str}")
                log_info(f"[Ê®°ÂºèÊ™¢Êü•] - ÊôÇÈñìÁØ©ÈÅ∏: {'ÂïìÁî®' if self.config.get('enable_time_filter', False) else 'ÈóúÈñâ'}")
                enable_limit = bool(self.config.get('enable_extract_count_limit', False))
                lim_n = int(self.config.get('extract_count', 0))
                if mode == 'qr_detection' and enable_limit: lim_n = int(self.config.get('qr_pages_per_archive', 10))
                log_info(f"[Ê®°ÂºèÊ™¢Êü•] - ÊèêÂèñÊï∏ÈáèÈôêÂà∂: {'ÂïìÁî® ('+str(lim_n)+'Âºµ)' if enable_limit else 'ÈóúÈñâ'}")
                log_info(f"[Ê®°ÂºèÊ™¢Êü•] ÂØ¶Èöõ‰ΩøÁî®ÁöÑÂúñÁâáÂø´Âèñ: {scan_cache_manager.cache_file_path}")
                log_info("=" * 50)
            except Exception as e: log_error(f"[Ê®°ÂºèÊ™¢Êü•] Ê®°ÂºèÊ©´ÂπÖÊó•Ë™åÁîüÊàêÂ§±Êïó: {e}")

            initial_files, self.vpath_size_map = get_files_to_process(self.config, scan_cache_manager, self.progress_queue, self.control_events)
            
            if self._check_control() == 'cancel': return None

            self.tasks_to_process = initial_files
            self.total_task_count = len(self.tasks_to_process)
            
            if not self.tasks_to_process:
                self.progress_queue.put({'type': 'text', 'text':"Âú®ÊåáÂÆöË∑ØÂæë‰∏ãÊú™ÊâæÂà∞‰ªª‰ΩïÁ¨¶ÂêàÊ¢ù‰ª∂ÁöÑÂúñÁâáÊ™îÊ°à„ÄÇ"})
                return [], {}, []
            
            mode = self.config.get('comparison_mode', 'mutual_comparison').lower()
            if mode == "qr_detection":
                if not QR_ENGINE_ENABLED:
                    log_error("QR ÂºïÊìé‰∏çÂèØÁî®ÔºåÁÑ°Ê≥ïÂü∑Ë°å QR Code Ê™¢Ê∏¨„ÄÇ")
                    return [], {}, [("Á≥ªÁµ±ÈåØË™§", "QR ÂºïÊìéÊú™ËºâÂÖ•")]
                result = self._detect_qr_codes(scan_cache_manager)
            else:
                ad_catalog_state = None
                if mode == 'ad_comparison':
                    ad_catalog_state = self._prepare_ad_catalog_epoch()

                result = self._find_similar_images(scan_cache_manager, ad_catalog_state)
                
            if result is None: return None
            found, data = result
            return found, data, self.failed_tasks
        finally:
            self._cleanup_pool()
    
    def _prepare_ad_catalog_epoch(self) -> Dict:
        """Ê™¢Êü•Âª£ÂëäÂ∫´ÁãÄÊÖãÔºåÂ¶ÇÊûúÁôºÁîüËÆäÊõ¥ÂâáÊèêÂçá epoch ÁâàÊú¨„ÄÇ"""
        ad_folder_path = self.config.get('ad_folder_path')
        if not ad_folder_path or not os.path.isdir(ad_folder_path):
            return {'epoch': 0, 'is_new': False}

        state_file = os.path.join(ad_folder_path, 'ad_catalog_state.json')
        folder_cache = FolderStateCacheManager(ad_folder_path)
        
        current_state = {'epoch': 1, 'last_build_mtime': 0}
        if os.path.exists(state_file):
            try:
                with open(state_file, 'r', encoding='utf-8') as f:
                    current_state = json.load(f)
            except (json.JSONDecodeError, IOError):
                pass # ‰ΩøÁî®È†êË®≠ÂÄºÈáçÂª∫

        # ‰ΩøÁî®Ë≥áÊñôÂ§æÁãÄÊÖãÂø´Âèñ‰æÜÂà§Êñ∑Âª£ÂëäÂ∫´ÊòØÂê¶ÊúâËÆäÊõ¥
        ad_root_state = folder_cache.get_folder_state(ad_folder_path)
        ad_root_mtime = os.path.getmtime(ad_folder_path) if ad_root_state else 0

        is_changed = False
        if not ad_root_state or abs(ad_root_mtime - ad_root_state.get('mtime', 0)) > 1e-6:
            is_changed = True
        
        if is_changed:
            log_info("[Epoch] Ê™¢Ê∏¨Âà∞Âª£ÂëäÂ∫´ÂÖßÂÆπËÆäÊõ¥ÔºåEpoch ÁâàÊú¨Â∞áÊèêÂçá„ÄÇ")
            current_state['epoch'] += 1
            current_state['last_build_mtime'] = ad_root_mtime
            try:
                with open(state_file, 'w', encoding='utf-8') as f:
                    json.dump(current_state, f, indent=2)
                folder_cache.update_folder_state(ad_folder_path, ad_root_mtime, os.path.getctime(ad_folder_path))
                folder_cache.save_cache()
            except IOError as e:
                log_error(f"ÁÑ°Ê≥ïÊõ¥Êñ∞Âª£ÂëäÂ∫´ÁãÄÊÖãÊ™îÊ°à: {e}")

        log_info(f"[Epoch] Áï∂ÂâçÂª£ÂëäÂ∫´ Epoch ÁâàÊú¨ÁÇ∫: {current_state['epoch']}")
        return current_state

    def _process_images_with_cache(self, current_task_list: list[str], cache_manager: ScannedImageCacheManager, description: str, worker_function: callable, data_key: str, progress_scope: str = 'global') -> tuple[bool, dict]:
        if not current_task_list: return True, {}
        local_file_data = {}
        time.sleep(self.config.get('ux_scan_start_delay', 0.1))
        self._update_progress(text=f"üìÇ Ê≠£Âú®Ê™¢Êü• {len(current_task_list)} ÂÄã{description}ÁöÑÂø´Âèñ...")
        
        local_total = len(current_task_list)
        local_completed = 0
        
        paths_to_recalc, cache_hits = [], 0
        folders_to_rescan = set()
        paths_to_purge = set()

        for path in list(current_task_list):
            cached_data = cache_manager.get_data(path)
            
            _, _, mt = _get_file_stat(path)
            
            if mt is None:
                if cached_data:
                    paths_to_purge.add(path)
                
                parent_folder = os.path.dirname(path if not _is_virtual_path(path) else _parse_virtual_path(path)[0])
                if os.path.isdir(parent_folder):
                    folders_to_rescan.add(parent_folder)
                
                if progress_scope == 'global':
                    self.total_task_count = max(0, self.total_task_count - 1)
                else:
                    local_total = max(0, local_total - 1)
                continue

            if cached_data:
                for hash_key in ['phash', 'whash']:
                    if hash_key in cached_data and cached_data[hash_key] and not isinstance(cached_data[hash_key], imagehash.ImageHash):
                        try: cached_data[hash_key] = imagehash.hex_to_hash(str(cached_data[hash_key]))
                        except (TypeError, ValueError): cached_data[hash_key] = None
            
            if cached_data and data_key in cached_data and cached_data.get(data_key) is not None and \
               abs(mt - float(cached_data.get('mtime', 0))) < 1e-6:
                local_file_data[path] = cached_data
                cache_hits += 1
                if progress_scope == 'global':
                    self.completed_task_count += 1
                else:
                    local_completed += 1
            else:
                paths_to_recalc.append(path)
                if cached_data: local_file_data[path] = cached_data

        if folders_to_rescan:
            log_info(f"Ê™¢Ê∏¨Âà∞ {len(folders_to_rescan)} ÂÄãË≥áÊñôÂ§æÂø´ÂèñÂ§±ÊïàÔºåÊ≠£Âú®ÈáçÊñ∞ÊéÉÊèè...")
            self._update_progress(text=f"‚ôªÔ∏è ÂÅµÊ∏¨Âà∞Âø´ÂèñÂ§±ÊïàÔºåÈáçÊñ∞ÊéÉÊèè {len(folders_to_rescan)} ÂÄãË≥áÊñôÂ§æ...")
            
            count = self.config.get('extract_count', 8)
            enable_limit = self.config.get('enable_extract_count_limit', True)
            
            for folder in sorted(list(folders_to_rescan)):
                try:
                    container_map = defaultdict(list)
                    for f in os.listdir(folder):
                        full_path = os.path.join(folder, f)
                        f_lower = f.lower()

                        if self.config.get("enable_archive_scan", False) and \
                           os.path.isfile(full_path) and \
                           os.path.splitext(f_lower)[1] in ('.zip', '.cbz', '.rar', '.cbr', '.7z'):
                            try:
                                all_vpaths = []
                                for ent in archive_handler.iter_archive_images(full_path):
                                    vpath = f"{config.VPATH_PREFIX}{ent.archive_path}{config.VPATH_SEPARATOR}{ent.inner_path}"
                                    all_vpaths.append(vpath)
                                all_vpaths.sort(key=_natural_sort_key)
                                take = all_vpaths[-count:] if enable_limit else all_vpaths
                                container_map[full_path].extend(take)
                            except Exception as e:
                                log_error(f"‰øùÂ∫ïÂ±ïÈñãÂ£ìÁ∏ÆÊ™îÂ§±Êïó: {full_path}: {e}", True)

                        elif f_lower.endswith(('.jpg', '.jpeg', '.png', '.gif', '.bmp', '.webp', '.tiff')):
                            container_map[folder].append(_norm_key(full_path))

                    for container, lst in container_map.items():
                        lst.sort(key=_natural_sort_key)
                        take = lst[-count:] if enable_limit else lst
                        for new_path in take:
                            if new_path not in paths_to_recalc and new_path not in local_file_data:
                                paths_to_recalc.append(new_path)
                                if progress_scope == 'global':
                                    self.total_task_count += 1
                                else:
                                    local_total += 1
                                log_info(f"  -> Êñ∞Â¢ûÊ™îÊ°àÈÄ≤Ë°åÂìàÂ∏åË®àÁÆó: {new_path}")
                except OSError as e:
                    log_error(f"ÈáçÊñ∞ÊéÉÊèèË≥áÊñôÂ§æ '{folder}' Â§±Êïó: {e}")

        if progress_scope == 'global':
            if self.total_task_count > 0:
                log_info(f"Âø´ÂèñÊ™¢Êü• - ÂëΩ‰∏≠: {cache_hits}/{self.total_task_count} | Á∏ΩÈÄ≤Â∫¶: {self.completed_task_count}/{self.total_task_count}")
        else:
            if local_total > 0:
                log_info(f"[Â±ÄÈÉ®] Âø´ÂèñÊ™¢Êü• - ÂëΩ‰∏≠: {cache_hits}/{local_total} | ÈÄ≤Â∫¶: {local_completed}/{local_total}")
        
        if paths_to_purge:
            log_info(f"Ê≠£Âú®ÂæûÂø´Âèñ‰∏≠ÊâπÊ¨°ÁßªÈô§ {len(paths_to_purge)} ÂÄãÁÑ°ÊïàÊ¢ùÁõÆ...")
            for path in paths_to_purge:
                cache_manager.remove_data(path)
        
        if not paths_to_recalc:
            cache_manager.save_cache()
            return True, local_file_data
        
        user_proc_setting = self.config.get('worker_processes', 0)
        pool_size = max(1, min(user_proc_setting, cpu_count())) if user_proc_setting > 0 else max(1, min(cpu_count() // 2, 8))
        if not self.pool:
            if sys.platform.startswith('win'):
                try: set_start_method('spawn', force=True)
                except RuntimeError: pass
            self.pool = Pool(processes=pool_size)
        self._update_progress(text=f"‚öôÔ∏è ‰ΩøÁî® {pool_size} ÈÄ≤Á®ãË®àÁÆó {len(paths_to_recalc)} ÂÄãÊñ∞Ê™îÊ°à...")
        async_results, path_map = [], {}
        worker_args = {}
        worker_name = worker_function.__name__
        if 'full' in worker_name or 'qr_code' in worker_name:
            worker_args['resize_size'] = self.config.get('qr_resize_size', 800)
        for path in paths_to_recalc:
            res = self.pool.apply_async(worker_function, args=(path,), kwds=worker_args)
            async_results.append(res)
            path_map[res] = path
        while async_results:
            if self._check_control() == 'cancel':
                self._cleanup_pool(); return False, {}
            remaining_results = []
            for res in async_results:
                if res.ready():
                    try:
                        path, data = res.get()
                        if data.get('error'): 
                            self.failed_tasks.append((path, data['error']))
                            if "‰∏çÂ≠òÂú®" in data['error']:
                                cache_manager.remove_data(path)
                        else:
                            local_file_data.setdefault(path, {}).update(data)
                            cache_manager.update_data(path, data)
                        
                        if progress_scope == 'global':
                            self.completed_task_count += 1
                        else:
                            local_completed += 1
                    except Exception as e:
                        path = path_map.get(res, "Êú™Áü•Ë∑ØÂæë")
                        error_msg = f"ÂæûÂ≠êÈÄ≤Á®ãÁç≤ÂèñÁµêÊûúÂ§±Êïó: {e}"
                        log_error(error_msg, True); self.failed_tasks.append((path, error_msg))
                        if progress_scope == 'global':
                            self.completed_task_count += 1
                        else:
                            local_completed += 1
                else: remaining_results.append(res)
            async_results = remaining_results

            if progress_scope == 'global':
                if self.total_task_count > 0:
                    current_progress = int(self.completed_task_count / self.total_task_count * 100)
                    self._update_progress(p_type='progress', value=current_progress, text=f"‚öôÔ∏è Ë®àÁÆó{description}‰∏≠... ({self.completed_task_count}/{self.total_task_count})")
            else:
                if local_total > 0:
                    current_progress = int(local_completed / local_total * 100)
                    self._update_progress(p_type='progress', value=current_progress, text=f"‚öôÔ∏è [Â±ÄÈÉ®] Ë®àÁÆó{description}‰∏≠... ({local_completed}/{local_total})")

            time.sleep(0.05)
        cache_manager.save_cache()
        return True, local_file_data

    def _build_phash_band_index(self, gallery_file_data: dict, bands=LSH_BANDS):
        seg_bits = HASH_BITS // bands
        mask = (1 << seg_bits) - 1
        index = [defaultdict(list) for _ in range(bands)]
        for path, ent in gallery_file_data.items():
            phash_obj = self._coerce_hash_obj(ent.get('phash'))
            if not phash_obj: continue
            try: v = int(str(phash_obj), 16)
            except (ValueError, TypeError): continue
            for b in range(bands):
                key = (v >> (b * seg_bits)) & mask
                index[b][key].append(_norm_key(path))
        return index

    def _lsh_candidates_for(self, ad_path: str, ad_hash_obj: imagehash.ImageHash, index: list, bands=LSH_BANDS):
        seg_bits = HASH_BITS // bands; mask = (1 << seg_bits) - 1
        v = int(str(ad_hash_obj), 16)
        cand = set()
        for b in range(bands):
            key = (v >> (b * seg_bits)) & mask
            cand.update(index[b].get(key, []))
        if _norm_key(ad_path) in cand: cand.remove(_norm_key(ad_path))
        return cand
        
    def _ensure_features(self, path: str, cache_mgr: ScannedImageCacheManager, need_hsv: bool = False, need_whash: bool = False) -> bool:
        norm_path = _norm_key(path)
        ent = self.file_data.get(norm_path)
        if not ent:
            ent = cache_mgr.get_data(norm_path) or {}
            self.file_data[norm_path] = ent
        
        if 'phash' in ent: ent['phash'] = self._coerce_hash_obj(ent['phash'])
        if 'whash' in ent: ent['whash'] = self._coerce_hash_obj(ent['whash'])
        if 'avg_hsv' in ent and isinstance(ent['avg_hsv'], list):
            try: ent['avg_hsv'] = tuple(float(x) for x in ent['avg_hsv'])
            except (ValueError, TypeError): ent['avg_hsv'] = None

        has_hsv = 'avg_hsv' in ent and ent['avg_hsv'] is not None
        has_whash = 'whash' in ent and ent['whash'] is not None
        
        if (not need_hsv or has_hsv) and (not need_whash or has_whash): return True

        img = None
        try:
            from PIL import Image, ImageOps
            img = _open_image_from_any_path(path)
            if not img: raise IOError("ÁÑ°Ê≥ïÈñãÂïüÂúñÁâá")
            
            img = ImageOps.exif_transpose(img)
            if need_hsv and not has_hsv: ent['avg_hsv'] = _avg_hsv(img)
            if need_whash and not has_whash and imagehash: ent['whash'] = imagehash.whash(img, hash_size=8, mode='haar', remove_max_haar_ll=True)
            
            _, _, mtime = _get_file_stat(path)
            update_payload = {'mtime': mtime}
            if 'avg_hsv' in ent and ent['avg_hsv'] is not None: update_payload['avg_hsv'] = list(ent['avg_hsv'])
            if 'whash' in ent and ent['whash'] is not None: update_payload['whash'] = str(ent['whash'])
            
            cache_mgr.update_data(norm_path, update_payload)
            return True
        except Exception as e:
            log_error(f"Êá∂Âä†ËºâÁâπÂæµÂ§±Êïó: {path}: {e}")
            return False
        finally:
            if img:
                try:
                    img.close()
                except Exception:
                    pass

    def _coerce_hash_obj(self, h):
        if h is None: return None
        if isinstance(h, imagehash.ImageHash): return h
        try: return imagehash.hex_to_hash(str(h))
        except (TypeError, ValueError): return None

    def _accept_pair_with_dual_hash(self, ad_hash_obj, g_hash_obj, ad_w_hash, g_w_hash) -> tuple[bool, float]:
        h1, h2 = self._coerce_hash_obj(ad_hash_obj), self._coerce_hash_obj(g_hash_obj)
        w1, w2 = self._coerce_hash_obj(ad_w_hash), self._coerce_hash_obj(g_w_hash)
        if not h1 or not h2: return False, 0.0
        
        sim_p = sim_from_hamming(h1 - h2, HASH_BITS)
        if sim_p < PHASH_FAST_THRESH: return False, sim_p
        if sim_p >= PHASH_STRICT_SKIP: return True, sim_p
        
        if not w1 or not w2: return False, sim_p
        
        sim_w = sim_from_hamming(w1 - w2, HASH_BITS)
        if sim_p >= 0.90:   ok = sim_w >= WHASH_TIER_1
        elif sim_p >= 0.88: ok = sim_w >= WHASH_TIER_2
        elif sim_p >= 0.85: ok = sim_w >= WHASH_TIER_3
        else:               ok = sim_w >= WHASH_TIER_4
        return (ok, min(sim_p, sim_w) if ok else sim_p)

    def _find_similar_images(self, scan_cache_manager: ScannedImageCacheManager, ad_catalog_state: Optional[Dict] = None) -> Union[tuple[list, dict], None]:
        tasks_to_process = self.tasks_to_process
        is_ad_mode = self.config.get('comparison_mode') == 'ad_comparison'
        current_epoch = 0
        
        if is_ad_mode and ad_catalog_state:
            current_epoch = ad_catalog_state.get('epoch', 0)
            
            unmatched_tasks = []
            for path in self.tasks_to_process:
                cached_data = scan_cache_manager.get_data(path)
                if not cached_data or cached_data.get('ad_epoch_done', 0) < current_epoch:
                    unmatched_tasks.append(path)
            
            log_info(f"[Epoch] Á∏Ω‰ªªÂãôÊï∏: {len(self.tasks_to_process)}, ÂÖ∂‰∏≠ {len(unmatched_tasks)} ÂÄã‰ªªÂãôÈúÄË¶ÅÈÄ≤Ë°åÂª£ÂëäÊØîÂ∞ç„ÄÇ")
            tasks_to_process = unmatched_tasks
            self.completed_task_count = len(self.tasks_to_process) - len(tasks_to_process)
            self.total_task_count = len(self.tasks_to_process)

        continue_processing, self.file_data = self._process_images_with_cache(tasks_to_process, scan_cache_manager, "ÁõÆÊ®ôÈõúÊπä", _pool_worker_process_image_phash_only, 'phash', progress_scope='global')
        if not continue_processing: return None
        
        gallery_data = {k: v for k, v in self.file_data.items() if k in tasks_to_process}
        
        user_thresh_percent = self.config.get('similarity_threshold', 95.0)
        is_mutual_mode = self.config.get('comparison_mode') == 'mutual_comparison'
        ad_folder_path = self.config.get('ad_folder_path'); ad_phash_set = set()
        
        if is_mutual_mode and self.config.get('enable_ad_cross_comparison', True):
            if ad_folder_path and os.path.isdir(ad_folder_path):
                self._update_progress(text="üì¶ Ê≠£Âú®È†êËôïÁêÜÂª£ÂëäÂ∫´...ÔºàÊ≠§ÈöéÊÆµÁÇ∫Â±ÄÈÉ®ÈÄ≤Â∫¶Ôºâ")
                ad_paths = []
                for ent in _iter_scandir_recursively(ad_folder_path, set(), set(), self.control_events):
                    if ent.is_file() and ent.name.lower().endswith(('.png','.jpg','.jpeg','.webp')):
                        ad_paths.append(_norm_key(ent.path))

                ad_cache = ScannedImageCacheManager(ad_folder_path, ad_folder_path, 'ad_comparison')
                _, ad_data_for_marking = self._process_images_with_cache(ad_paths, ad_cache, "È†êËºâÂÖ•Âª£ÂëäÂ∫´", _pool_worker_process_image_phash_only, 'phash', progress_scope='local')
                if ad_data_for_marking:
                    for data in ad_data_for_marking.values():
                        phash = self._coerce_hash_obj(data.get('phash'))
                        if phash: ad_phash_set.add(phash)
                    log_info(f"[‰∫§ÂèâÊØîÂ∞ç] ÊàêÂäüÂæûÊ™îÊ°àÈ†êËºâÂÖ• {len(ad_phash_set)} ÂÄãÂª£ÂëäÂìàÂ∏å„ÄÇ")
            if self.config.get('cross_comparison_include_bw', False) and imagehash:
                log_info("[‰∫§ÂèâÊØîÂ∞ç] Â∑≤ÂïüÁî®Â∞çÁ¥îÈªë/Á¥îÁôΩÂúñÁâáÁöÑÊô∫ÊÖßÊ®ôË®ò„ÄÇ")
                ad_phash_set.add(imagehash.hex_to_hash('0000000000000000')); ad_phash_set.add(imagehash.hex_to_hash('ffffffffffffffff'))
                
                try:
                    ad_int_set = {int(str(h), 16) for h in ad_phash_set}
                except Exception:
                    ad_int_set = set()
                _ad_cross_sim = 0.98
                _ad_dmax = hamming_from_sim(_ad_cross_sim, HASH_BITS)
                _ONEBIT_MASKS = [1 << i for i in range(HASH_BITS)]
                
        def lerp(p, s, l): return s + ((100.0 - max(80.0, min(100.0, float(p)))) / 20.0) * (l - s)
        color_gate_params = { 'hue_deg_tol': lerp(user_thresh_percent, 18.0, 25.0), 'sat_tol': lerp(user_thresh_percent, 0.18, 0.25),
            'low_sat_value_tol': lerp(user_thresh_percent, 0.10, 0.15), 'low_sat_achroma_tol': lerp(user_thresh_percent, 0.12, 0.18),
            'low_sat_thresh': 0.18 }
        
        ad_data, ad_cache_manager, leader_to_ad_group = {}, None, {}
        
        if is_ad_mode:
            self._update_progress(text="üì¶ Ê≠£Âú®È†êËôïÁêÜÂª£ÂëäÂ∫´...ÔºàÊ≠§ÈöéÊÆµÁÇ∫Â±ÄÈÉ®ÈÄ≤Â∫¶Ôºâ")
            ad_paths = []
            for ent in _iter_scandir_recursively(ad_folder_path, set(), set(), self.control_events):
                if ent.is_file() and ent.name.lower().endswith(('.png','.jpg','.jpeg','.webp')):
                    ad_paths.append(_norm_key(ent.path))
            ad_cache_manager = ScannedImageCacheManager(ad_folder_path, ad_folder_path, 'ad_comparison')
            continue_proc_ad, ad_data = self._process_images_with_cache(ad_paths, ad_cache_manager, "Âª£ÂëäÂúñÁâáÈõúÊπä", _pool_worker_process_image_phash_only, 'phash', progress_scope='local')
            if not continue_proc_ad: return None
            self.file_data.update(ad_data)
            self._update_progress(text="üîç Ê≠£Âú®‰ΩøÁî® LSH È´òÊïàÈ†êËôïÁêÜÂª£ÂëäÂ∫´...")
            ad_lsh_index = self._build_phash_band_index(ad_data)
            ad_path_to_leader = {p: p for p in ad_data}; ad_paths_sorted = sorted(list(ad_data.keys()))
            grouping_dist = hamming_from_sim(AD_GROUPING_THRESHOLD, HASH_BITS)
            for p1 in ad_paths_sorted:
                if ad_path_to_leader[p1] != p1: continue
                h1 = self._coerce_hash_obj(ad_data.get(p1, {}).get('phash'))
                if not h1: continue
                for p2 in self._lsh_candidates_for(p1, h1, ad_lsh_index):
                    if p2 <= p1 or ad_path_to_leader[p2] != p2: continue
                    h2 = self._coerce_hash_obj(ad_data.get(p2, {}).get('phash'))
                    if h2 and (h1 - h2) <= grouping_dist: ad_path_to_leader[p2] = ad_path_to_leader[p1]
            for path, leader in ad_path_to_leader.items(): leader_to_ad_group.setdefault(leader, []).append(path)
            ad_data_representatives = {p: d for p, d in ad_data.items() if p in leader_to_ad_group}
            self._update_progress(text=f"üîç Âª£ÂëäÂ∫´È†êËôïÁêÜÂÆåÊàêÔºåÊâæÂà∞ {len(ad_data_representatives)} ÂÄãÁç®Á´ãÂª£ÂëäÁµÑ„ÄÇ")
        else: 
            ad_data_representatives = gallery_data.copy()
            
        phash_index = self._build_phash_band_index(gallery_data)
        temp_found_pairs, user_thresh = [], user_thresh_percent / 100.0
        inter_folder_only = self.config.get('enable_inter_folder_only', False) and is_mutual_mode
        stats = {'comparisons': 0, 'passed_phash': 0, 'passed_color': 0, 'entered_whash': 0, 'filtered_inter': 0}
        
        for i, (p1_path, p1_ent) in enumerate(ad_data_representatives.items()):
            if self._check_control() != 'continue': return None
            p1_p_hash = self._coerce_hash_obj(p1_ent.get('phash'))
            if not p1_p_hash: continue
            
            for p2_path in self._lsh_candidates_for(p1_path, p1_p_hash, phash_index):
                if is_mutual_mode:
                    if p2_path <= p1_path: continue
                    if inter_folder_only:
                        p1_parent_base = p1_path if not _is_virtual_path(p1_path) else _parse_virtual_path(p1_path)[0]
                        p2_parent_base = p2_path if not _is_virtual_path(p2_path) else _parse_virtual_path(p2_path)[0]
                        if os.path.dirname(p1_parent_base) == os.path.dirname(p2_parent_base): 
                            stats['filtered_inter'] += 1; continue
                if is_ad_mode and p2_path in ad_data: continue
                
                is_match, best_sim = False, 0.0
                ad_group = leader_to_ad_group.get(p1_path, [p1_path])
                
                for member_path in ad_group:
                    stats['comparisons'] += 1
                    
                    h1_raw = (gallery_data if is_mutual_mode else ad_data).get(member_path, {}).get('phash')
                    h2_raw = gallery_data.get(p2_path, {}).get('phash')
                    
                    h1 = self._coerce_hash_obj(h1_raw)
                    h2 = self._coerce_hash_obj(h2_raw)
                    if not h1 or not h2: continue
                    
                    sim_p = sim_from_hamming(h1 - h2, HASH_BITS)
                    if sim_p < PHASH_FAST_THRESH: continue
                    stats['passed_phash'] += 1
                    
                    if self.config.get('enable_color_filter', True):
                        cache1 = scan_cache_manager if is_mutual_mode else ad_cache_manager
                        if not self._ensure_features(member_path, cache1, need_hsv=True) or \
                           not self._ensure_features(p2_path, scan_cache_manager, need_hsv=True): continue
                        
                        hsv1 = self.file_data[_norm_key(member_path)].get('avg_hsv')
                        hsv2 = self.file_data[_norm_key(p2_path)].get('avg_hsv')
                        if not _color_gate(hsv1, hsv2, **color_gate_params): continue
                    
                    stats['passed_color'] += 1

                    accepted, final_sim = True, sim_p
                    if sim_p < PHASH_STRICT_SKIP:
                        stats['entered_whash'] += 1
                        cache1 = scan_cache_manager if is_mutual_mode else ad_cache_manager
                        if not self._ensure_features(member_path, cache1, need_whash=True) or \
                           not self._ensure_features(p2_path, scan_cache_manager, need_whash=True): continue
                        
                        w1 = self.file_data[_norm_key(member_path)].get('whash')
                        w2 = self.file_data[_norm_key(p2_path)].get('whash')
                        accepted, final_sim = self._accept_pair_with_dual_hash(h1, h2, w1, w2)
                    
                    if accepted and final_sim >= user_thresh:
                        is_match, best_sim = True, max(best_sim, final_sim)

                if is_match:
                    temp_found_pairs.append((p1_path, p2_path, f"{best_sim * 100:.1f}%"))
        
        found_items = []
        if is_mutual_mode:
            self._update_progress(text="üîÑ Ê≠£Âú®Âêà‰ΩµÁõ∏‰ººÁæ£ÁµÑ...")
            path_to_leader = {}
            sorted_pairs = sorted([(_norm_key(p1), _norm_key(p2), sim) for p1, p2, sim in temp_found_pairs], key=lambda x: (min(x[0], x[1]), max(x[0], x[1])))
            for p1, p2, _ in sorted_pairs:
                l1, l2 = path_to_leader.get(p1, p1), path_to_leader.get(p2, p2)
                if l1 != l2:
                    final_l = min(l1, l2)
                    path_to_leader[l1] = path_to_leader[l2] = final_l
                    path_to_leader[p1] = path_to_leader[p2] = final_l
            final_groups = defaultdict(list)
            all_paths_in_pairs = {_norm_key(p) for pair in temp_found_pairs for p in pair[:2]}
            for path in all_paths_in_pairs:
                leader = path
                while leader in path_to_leader and path_to_leader[leader] != leader: leader = path_to_leader[leader]
                final_groups[leader].append(path)
            
            if ad_phash_set:
                self._update_progress(text="üîÑ Ê≠£Âú®ËàáÂª£ÂëäÂ∫´ÈÄ≤Ë°å‰∫§ÂèâÊØîÂ∞ç...")
                if '_ad_dmax' in locals() and '_ONEBIT_MASKS' in locals() and '_ad_int_set' not in locals():
                    ad_int_set = locals().get('ad_int_set', set())
                if '_ad_dmax' in locals() and _ad_dmax <= 1 and 'ad_int_set' in locals() and ad_int_set:
                    for leader, children in final_groups.items():
                        is_ad_like = False
                        for p in [leader] + children:
                            h_obj = self._coerce_hash_obj(self.file_data.get(_norm_key(p), {}).get('phash'))
                            if not h_obj: continue
                            v = int(str(h_obj), 16)
                            if v in ad_int_set:
                                is_ad_like = True; break
                            hit = False
                            for m in _ONEBIT_MASKS:
                                if (v ^ m) in ad_int_set:
                                    hit = True; break
                            if hit:
                                is_ad_like = True; break
                        for child in sorted([p for p in children if p != leader]):
                            h1, h2 = self._coerce_hash_obj(self.file_data.get(_norm_key(leader), {}).get('phash')), \
                                     self._coerce_hash_obj(self.file_data.get(_norm_key(child), {}).get('phash'))
                            if h1 and h2:
                                sim = sim_from_hamming(h1 - h2, HASH_BITS) * 100
                                value_str = f"{sim:.1f}%" + (" (‰ººÂª£Âëä)" if is_ad_like else "")
                                found_items.append((leader, child, value_str))
                else:
                    ad_match_thresh = hamming_from_sim(0.98, HASH_BITS)
                    for leader, children in final_groups.items():
                        is_ad_like = False
                        group_hashes = {self._coerce_hash_obj(self.file_data.get(_norm_key(p), {}).get('phash')) for p in [leader] + children}
                        group_hashes.discard(None)
                        if any(h and any((h - ah) <= ad_match_thresh for ah in ad_phash_set) for h in group_hashes):
                            is_ad_like = True
                        for child in sorted([p for p in children if p != leader]):
                            h1, h2 = self._coerce_hash_obj(self.file_data.get(_norm_key(leader), {}).get('phash')), \
                                     self._coerce_hash_obj(self.file_data.get(_norm_key(child), {}).get('phash'))
                            if h1 and h2:
                                sim = sim_from_hamming(h1 - h2, HASH_BITS) * 100
                                value_str = f"{sim:.1f}%" + (" (‰ººÂª£Âëä)" if is_ad_like else "")
                                found_items.append((leader, child, value_str))
            else:
                for leader, children in final_groups.items():
                    for child in sorted([p for p in children if p != leader]):
                        h1, h2 = self.file_data.get(_norm_key(leader), {}).get('phash'), self.file_data.get(_norm_key(child), {}).get('phash')
                        if h1 and h2:
                            sim = sim_from_hamming(self._coerce_hash_obj(h1) - self._coerce_hash_obj(h2), HASH_BITS) * 100
                            found_items.append((leader, child, f"{sim:.1f}%"))
        else:
            results_by_leader = defaultdict(list)
            for leader, target, sim_str in temp_found_pairs: results_by_leader[leader].append((target, float(sim_str.replace('%','')), sim_str))
            for leader, targets in results_by_leader.items():
                for target, _, sim_str in sorted(targets, key=lambda x: x[1], reverse=True): found_items.append((leader, target, sim_str))
        
        log_performance("[ÂÆåÊàê] LSH ÈõôÂìàÂ∏åÊØîÂ∞çÈöéÊÆµ")
        log_info("--- ÊØîÂ∞çÂºïÊìéÊºèÊñóÁµ±Ë®à ---")
        if stats['filtered_inter'] > 0: 
            log_info(f"Âõ†\"ÂÉÖÊØîÂ∞ç‰∏çÂêåË≥áÊñôÂ§æ\"ËÄåË∑≥ÈÅé: {stats['filtered_inter']:,} Ê¨°")
        log_info(f"Âª£ÂëäÁµÑÂ±ïÈñãÂæåÁ∏ΩÊØîÂ∞çÊ¨°Êï∏: {stats['comparisons']:,}")
        passed_phash, passed_color, entered_whash = stats['passed_phash'], stats['passed_color'], stats['entered_whash']
        if stats['comparisons'] > 0: 
            log_info(f"ÈÄöÈÅé pHash Âø´ÁØ©: {passed_phash:,} ({(passed_phash/stats['comparisons']*100 if stats['comparisons'] > 0 else 0):.1f}%)")
        if passed_phash > 0: 
            log_info(f" ‚îî‚îÄ ÈÄöÈÅéÈ°èËâ≤ÈÅéÊøæÈñò: {passed_color:,} ({(passed_color/passed_phash*100 if passed_phash > 0 else 0):.1f}%)")
        if passed_color > 0: 
            log_info(f"    ‚îî‚îÄ ÈÄ≤ÂÖ• wHash Ë§áÊ†∏: {entered_whash:,} ({(entered_whash/passed_color*100 if passed_color > 0 else 0):.1f}%)")
        final_matches = len({(_norm_key(p1), _norm_key(p2)) for p1, p2, _ in temp_found_pairs})
        if passed_color > 0: 
            log_info(f"       ‚îî‚îÄ ÊúÄÁµÇÊúâÊïàÂåπÈÖç: {final_matches:,} ({(final_matches/passed_color*100 if passed_color > 0 else 0):.1f}%)")
        log_info("--------------------------")
        
        full_gallery_data = self.file_data
        self.file_data = {**full_gallery_data, **ad_data}
        
        if is_ad_mode and current_epoch > 0:
            log_info(f"[Epoch] Ê≠£Âú®ÁÇ∫ {len(tasks_to_process)} ÂÄãÂ∑≤ËôïÁêÜÁöÑÂúñÁâáÊõ¥Êñ∞ Epoch Ê®ôË®òËá≥ÁâàÊú¨ {current_epoch}...")
            for path in tasks_to_process:
                norm_path = _norm_key(path)
                if norm_path in self.file_data:
                    self.file_data[norm_path]['ad_epoch_done'] = current_epoch
                scan_cache_manager.update_data(norm_path, {'ad_epoch_done': current_epoch})
            scan_cache_manager.save_cache()
            log_info("[Epoch] Epoch Ê®ôË®òÊõ¥Êñ∞ÂÆåÊàê„ÄÇ")

        return found_items, self.file_data

    def _detect_qr_codes(self, scan_cache_manager: ScannedImageCacheManager) -> Union[tuple[list, dict], None]:
        if self.config.get('enable_qr_hybrid_mode'):
            return self._detect_qr_codes_hybrid(self.tasks_to_process, scan_cache_manager)
        else:
            return self._detect_qr_codes_pure(self.tasks_to_process, scan_cache_manager)

    def _detect_qr_codes_pure(self, files_to_process: list[str], scan_cache_manager: ScannedImageCacheManager) -> Union[tuple[list, dict], None]:
        log_info("[QR] Ê≠£Âú®Âü∑Ë°åÁ¥îÁ≤πÊéÉÊèèÊ®°Âºè...")
        continue_processing, file_data = self._process_images_with_cache(
            files_to_process, scan_cache_manager, "QR Code Ê™¢Ê∏¨", 
            _pool_worker_detect_qr_code, 'qr_points'
        )
        if not continue_processing: return None
        found_qr_images = [(path, path, "QR Code Ê™¢Âá∫") for path, data in file_data.items() if data and data.get('qr_points')]
        self.file_data = file_data
        return found_qr_images, self.file_data

    def _detect_qr_codes_hybrid(self, files_to_process: list[str], scan_cache_manager: ScannedImageCacheManager) -> Union[tuple[list, dict], None]:
        log_info("[QR] Ê≠£Âú®Âü∑Ë°åÊ∑∑ÂêàÊéÉÊèèÊ®°Âºè...")
        ad_folder_path = self.config.get('ad_folder_path')
        if not ad_folder_path or not os.path.isdir(ad_folder_path):
            log_info("[QRÊ∑∑ÂêàÊ®°Âºè] Âª£ÂëäË≥áÊñôÂ§æÁÑ°ÊïàÔºåÈÄÄÂõûÁ¥îÁ≤πÊéÉÊèèÊ®°Âºè„ÄÇ")
            return self._detect_qr_codes_pure(files_to_process, scan_cache_manager)
        
        self._update_progress(text="üì¶ Ê≠£Âú®È†êËôïÁêÜÂª£ÂëäÂ∫´...ÔºàÊ≠§ÈöéÊÆµÁÇ∫Â±ÄÈÉ®ÈÄ≤Â∫¶Ôºâ")
        ad_paths = []
        for ent in _iter_scandir_recursively(ad_folder_path, set(), set(), self.control_events):
            if ent.is_file() and ent.name.lower().endswith(('.png','.jpg','.jpeg','.webp')):
                ad_paths.append(_norm_key(ent.path))

        ad_cache_manager = ScannedImageCacheManager(ad_folder_path, ad_folder_path, 'ad_comparison')
        continue_proc_ad, ad_data = self._process_images_with_cache(
            ad_paths, ad_cache_manager, "Âª£ÂëäÂúñÁâáÂ±¨ÊÄß", 
            _pool_worker_process_image_full, 'qr_points', progress_scope='local'
        )
        if not continue_proc_ad: return None
        self.file_data.update(ad_data)
        ad_with_phash = {path: data for path, data in ad_data.items() if data and data.get('phash')}
        if not ad_with_phash:
            log_info("[QRÊ∑∑ÂêàÊ®°Âºè] Âª£ÂëäË≥áÊñôÂ§æÁÑ°ÊúâÊïàÂìàÂ∏åÔºåÈÄÄÂõûÁ¥îÁ≤πÊéÉÊèèÊ®°Âºè„ÄÇ")
            return self._detect_qr_codes_pure(files_to_process, scan_cache_manager)
        continue_proc_gallery, gallery_data = self._process_images_with_cache(
            files_to_process, scan_cache_manager, "ÁõÆÊ®ôÈõúÊπä", 
            _pool_worker_process_image_phash_only, 'phash', progress_scope='global'
        )
        if not continue_proc_gallery: return None
        self.file_data.update(gallery_data)
        self._update_progress(text="üîç [Ê∑∑ÂêàÊ®°Âºè] Ê≠£Âú®‰ΩøÁî® LSH Âø´ÈÄüÂåπÈÖçÂª£Âëä...")
        phash_index = self._build_phash_band_index(gallery_data)
        found_ad_matches = []
        user_thresh = self.config.get('similarity_threshold', 95.0) / 100.0
        for ad_path, ad_ent in ad_with_phash.items():
            if self._check_control() != 'continue': return None
            ad_p_hash = self._coerce_hash_obj(ad_ent.get('phash'))
            if not ad_p_hash: continue
            candidate_paths = self._lsh_candidates_for(ad_path, ad_p_hash, phash_index)
            for g_path in candidate_paths:
                g_p_hash = self._coerce_hash_obj(gallery_data.get(g_path, {}).get('phash'))
                if not g_p_hash: continue

                sim_p = sim_from_hamming(ad_p_hash - g_p_hash, HASH_BITS)
                if sim_p < PHASH_FAST_THRESH: continue
                
                is_accepted, final_sim_val = True, sim_p
                if sim_p < PHASH_STRICT_SKIP:
                    if not self._ensure_features(ad_path, ad_cache_manager, need_whash=True) or \
                       not self._ensure_features(g_path, scan_cache_manager, need_whash=True): continue
                    ad_w_hash = self.file_data.get(_norm_key(ad_path),{}).get('whash')
                    g_w_hash = self.file_data.get(_norm_key(g_path),{}).get('whash')
                    is_accepted, final_sim_val = self._accept_pair_with_dual_hash(ad_p_hash, g_p_hash, ad_w_hash, g_w_hash)
                
                if is_accepted and final_sim_val >= user_thresh and ad_ent.get('qr_points'):
                    found_ad_matches.append((ad_path, g_path, "Âª£ÂëäÂåπÈÖç(Âø´ÈÄü)"))
                    gallery_data.setdefault(g_path, {})['qr_points'] = ad_ent['qr_points']
                        
        matched_gallery_paths = {_norm_key(pair[1]) for pair in found_ad_matches}
        remaining_files_for_qr = [path for path in self.tasks_to_process if _norm_key(path) not in matched_gallery_paths]
        
        self._update_progress(text=f"üîç Â∞çÂâ©È§ò {len(remaining_files_for_qr)} ÂÄãÊ™îÊ°àÈÄ≤Ë°å QR ÊéÉÊèèÔºàÂ±ÄÈÉ®ÈÄ≤Â∫¶Ôºâ")
        if remaining_files_for_qr:
            if self._check_control() != 'continue': return None
            continue_proc_qr, qr_data = self._process_images_with_cache(
                remaining_files_for_qr, scan_cache_manager, "QR Code Ê™¢Ê∏¨", 
                _pool_worker_detect_qr_code, 'qr_points', progress_scope='local'
            )
            if not continue_proc_qr: return None
            qr_results = [(path, path, "QR Code Ê™¢Âá∫") for path, data in qr_data.items() if data and data.get('qr_points')]
            found_ad_matches.extend(qr_results)
            self.file_data.update(qr_data)
        scan_cache_manager.save_cache()
        ad_cache_manager.save_cache()
        return found_ad_matches, self.file_data