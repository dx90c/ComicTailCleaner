# ======================================================================
# æª”æ¡ˆï¼šplugins/eh_database_tools/processor.py
# ç›®çš„ï¼šå¯¦ç¾ä¸€å€‹ã€Œå‰ç½®è™•ç†å™¨ã€ï¼Œåœ¨ä¸»ä»»å‹™å‰åŒæ­¥ EH è³‡æ–™åº«
# ç‰ˆæœ¬ï¼š1.9.10 (ç©©å®šæ€§ä¿®æ­£ï¼šç¢ºä¿é€²ç¨‹æª¢æŸ¥çµ•ä¸ä¸­æ–·ä¸»æµç¨‹)
# ======================================================================
# v1.9.10 æ›´æ–°æ—¥èªŒ:
#   1. ã€é˜²å‘†æ©Ÿåˆ¶ã€‘: å°‡ close_manga_app_if_running å‡½å¼å…§éƒ¨é‚è¼¯å®Œå…¨åŒ…è£¹åœ¨
#      try...except å€å¡Šä¸­ï¼Œç¢ºä¿å³ä½¿ psutil ç™¼ç”ŸéŒ¯èª¤æˆ–æ¬Šé™å•é¡Œï¼Œ
#      ç¨‹å¼ä¹Ÿåªæœƒè¨˜éŒ„è­¦å‘Šä¸¦ã€Œå¼·åˆ¶ç¹¼çºŒã€åŸ·è¡Œä¸‹ä¸€æ­¥ï¼Œè§£æ±ºå¡ä½å•é¡Œã€‚
#   2. ã€ç‹€æ…‹å›é¥‹ã€‘: åœ¨æª¢æŸ¥çµæŸå¾Œç«‹å³æ›´æ–° UI é€²åº¦ï¼Œçµ¦äºˆä½¿ç”¨è€…æ˜ç¢ºçš„ç¹¼çºŒè¨Šè™Ÿã€‚
# ======================================================================

from __future__ import annotations
import os
import sqlite3
import hashlib
import datetime
import json
import time
import re
import shutil
from tqdm import tqdm
from typing import Dict, Any, Tuple, List, Optional, Union, Sequence, Iterable
from pathlib import Path
import io
import csv
import tempfile
import subprocess
import threading
import queue

from plugins.base_plugin import BasePlugin
from utils import log_info, log_error, log_warning
from config import DATA_DIR

try:
    import pyautogui, keyboard, psutil, pyperclip, ctypes
    from ctypes import wintypes
    from PIL import Image
    AUTOMATION_LIBS_AVAILABLE = True
except ImportError:
    AUTOMATION_LIBS_AVAILABLE = False

GLOBAL_ARTIST_MAP = {}
GLOBAL_GROUP_MAP = {}
summary = None
PLUGIN_ROOT_PATH = os.path.dirname(os.path.abspath(__file__))

PAGE_LOAD_DELAY = 2.0;
SEARCH_BOX_X_OFFSET = -100; TITLE_X_OFFSET = -100; TITLE_Y_OFFSET = -20
MAIN_SEARCH_ICON_IMG, BOOKMARK_ICON_IMG, BOOKMARK_ICON_READY_IMG, RESCAN_BUTTON_IMG, CLOSE_BUTTON_IMG, PAGE_END_IMG, CLEAR_SEARCH_BUTTON_IMG, NO_COVER_IMG = 'main_search_icon.png', 'bookmark_icon.png', 'bookmark_icon_ready.png', 'rescan_button.png', 'close_button.png', 'page_end.png', 'clear_search_button.png', 'no_cover.png'

SPEED_PRESETS = {
    "safe":   {"PAUSE": 0.35, "CLICK": 0.30, "PAGEDOWN": 0.15, "AFTER_SCROLL": 0.25},
    "normal": {"PAUSE": 0.20, "CLICK": 0.18, "PAGEDOWN": 0.10, "AFTER_SCROLL": 0.15},
    "fast":   {"PAUSE": 0.05, "CLICK": 0.08, "PAGEDOWN": 0.08, "AFTER_SCROLL": 0.10},
}

def _init_automation_speed_from_config(config: dict):
    speed = (config or {}).get("automation_speed", "fast").strip().lower()
    timing = SPEED_PRESETS.get(speed, SPEED_PRESETS["fast"])
    if AUTOMATION_LIBS_AVAILABLE:
        pyautogui.PAUSE = timing["PAUSE"]
    return timing

class ExecutionSummary:
    def __init__(self):
        self.start_time = time.time(); self.end_time = None; self.mode = "æœªçŸ¥"
        self.added = 0; self.soft_deleted = 0; self.restored = 0
        self.moved_empty = 0; self.tasks_total = 0; self.tasks_processed = 0
    def finish(self): self.end_time = time.time()
    def report(self):
        if not self.end_time: self.finish()
        duration = self.end_time - self.start_time; mins, secs = divmod(duration, 60)
        report_lines = ["\n", "="*70, f"[EH å¤–æ›] åŸ·è¡Œæ‘˜è¦å ±å‘Š (v29.1 æ ¸å¿ƒ)", "="*70, f"åŸ·è¡Œæ¨¡å¼: {self.mode}", f"æ­·æ™‚ {int(mins)}åˆ† {int(secs)}ç§’", "--- è³‡æ–™åº«åŒæ­¥æˆæœ ---", f"    [+] æ–°å¢è¨˜éŒ„: {self.added} ç­†", f"    [-] è»Ÿåˆªé™¤è¨˜éŒ„: {self.soft_deleted} ç­†", f"    [*] é‚„åŸè¨˜éŒ„: {self.restored} ç­†", f"    [+] ç§»å‹•ç©ºè³‡æ–™å¤¾: {self.moved_empty} å€‹", "--- UI è‡ªå‹•åŒ–æˆæœ ---", f"    [*] å¾…è™•ç†ä»»å‹™ç¸½æ•¸: {self.tasks_total} å€‹", f"    [âˆš] æˆåŠŸè™•ç†ä»»å‹™: {self.tasks_processed} å€‹", "="*70]
        for line in report_lines: log_info(line)

def normalize_path(path: str) -> str:
    if not path: return ""
    return os.path.normpath(path).replace('\\', '/')

def sanitize_filename(name: str) -> str:
    name = re.sub(r'[<>:"/\\|?*]', '_', name); name = name.strip('. '); return name

def add_normalized_path_column_if_not_exists(db_path: str):
    with sqlite3.connect(db_path) as conn:
        if 'filepath_normalized' not in [info[1] for info in conn.execute("PRAGMA table_info(Mangas)")]:
            log_info("[EH å¤–æ›] åµæ¸¬åˆ°èˆŠç‰ˆè³‡æ–™åº«ï¼Œæ­£åœ¨æ–°å¢ 'filepath_normalized' æ¬„ä½...")
            conn.execute("ALTER TABLE Mangas ADD COLUMN filepath_normalized TEXT")
            log_info("  -> æ¬„ä½æ–°å¢å®Œæˆã€‚")

def migrate_to_v20_structure(db_path: str):
    with sqlite3.connect(db_path) as conn:
        conn.executemany("UPDATE Mangas SET filepath = ? WHERE id = ?", [(path.replace('/', '\\'), pid) for pid, path in conn.execute("SELECT id, filepath FROM Mangas WHERE filepath LIKE '%/%'")])
        records_to_migrate = list(conn.execute("SELECT id, filepath FROM Mangas WHERE filepath_normalized IS NULL OR filepath_normalized = '' OR filepath_normalized LIKE '%\\%'"))
        if records_to_migrate:
            log_info(f"[EH å¤–æ›] æ­£åœ¨é·ç§» {len(records_to_migrate)} ç­†è¨˜éŒ„åˆ°æ–°çš„è·¯å¾‘æ¨™æº–...")
            conn.executemany("UPDATE Mangas SET filepath_normalized = ? WHERE id = ?", [(normalize_path(path), pid) for pid, path in records_to_migrate])
            log_info("  -> è·¯å¾‘é·ç§»å®Œæˆã€‚")

def load_maps_from_ast_json(filepath: str) -> Tuple[Dict[str, str], Dict[str, str]]:
    def first_text_in(obj: Any) -> Union[str, None]:
        if isinstance(obj, dict):
            if obj.get("type") == "text" and isinstance(obj.get("text"), str): return obj["text"]
            for v in obj.values():
                found = first_text_in(v)
                if isinstance(found, str) and found.strip(): return found
        elif isinstance(obj, list):
            for item in obj:
                found = first_text_in(item)
                if isinstance(found, str) and found.strip(): return found
        return None
    artist_map, group_map = {}, {}
    try:
        with open(filepath, "r", encoding="utf-8") as f: root = json.load(f)
        sections = root.get("data")
        if not isinstance(sections, list): return {}, {}
        for ns in ("artist", "group"):
            section = next((s for s in sections if isinstance(s, dict) and s.get("namespace") == ns), None)
            if not section: continue
            data_block = section.get("data")
            if not isinstance(data_block, dict): continue
            target_map = artist_map if ns == "artist" else group_map
            for raw_tag, entry_data in data_block.items():
                if not isinstance(entry_data, dict): continue
                key_japanese = first_text_in(entry_data.get("name"))
                value_romaji = raw_tag.replace('_', ' ').title()
                if isinstance(key_japanese, str) and key_japanese.strip() and value_romaji:
                    target_map[key_japanese.strip().lower()] = value_romaji
    except Exception as e: log_error(f"[EH å¤–æ›] è§£æ EhTag è³‡æ–™åº«æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}"); return {}, {}
    return artist_map, group_map

def load_translation_maps(config: Dict):
    global GLOBAL_ARTIST_MAP, GLOBAL_GROUP_MAP
    log_info("[EH å¤–æ›] æ­£åœ¨è¼‰å…¥ EhTag é›™è»Œç¿»è­¯è³‡æ–™åº«...")
    ehtag_db_dir = config.get('eh_syringe_directory')
    if not ehtag_db_dir or not os.path.isdir(ehtag_db_dir):
        log_warning("[EH å¤–æ›] æœªè¨­å®šæœ‰æ•ˆçš„ EhTag DB è·¯å¾‘ï¼Œå°‡è·³éç¿»è­¯èˆ‡ CSV åŠŸèƒ½ã€‚"); return
    db_path = os.path.join(ehtag_db_dir, 'db.ast.json')
    if not os.path.exists(db_path): log_error(f"[EH å¤–æ›] æ‰¾ä¸åˆ°è³‡æ–™åº«æª”æ¡ˆ: {db_path}"); return
    GLOBAL_ARTIST_MAP, GLOBAL_GROUP_MAP = load_maps_from_ast_json(db_path)
    log_info(f"  -> Artist è³‡æ–™åº«è¼‰å…¥å®Œæˆ: {len(GLOBAL_ARTIST_MAP)} ç­†")
    log_info(f"  -> Group è³‡æ–™åº«è¼‰å…¥å®Œæˆ: {len(GLOBAL_GROUP_MAP)} ç­†")

def is_romaji_candidate(text: str) -> bool:
    return all(ord(c) < 128 for c in text.replace(' ', '').replace('_', '').replace('-', ''))

def analyze_title_tags(title: str) -> Tuple[str, str]:
    if not title: return "", ""
    artist_val, group_val = "", ""
    matches = re.findall(r'\[([^\]]+)\]', title)
    for content in matches:
        content = content.strip(); content_lower = content.lower()
        if content_lower in ['chinese', 'dlç‰ˆ', 'ä¸­å›½ç¿»è¨³', 'ç¿»è¨³', 'ç„¡ä¿®æ­£', 'uncensored']: continue
        inner_match = re.search(r'[(ï¼ˆ]([^)ï¼‰]+)[)ï¼‰]', content)
        if inner_match:
            inner_artist = inner_match.group(1).strip()
            outer_group = re.split(r'[(ï¼ˆ]', content)[0].strip()
            if not artist_val:
                artist_val = GLOBAL_ARTIST_MAP.get(inner_artist.lower())
                if not artist_val and is_romaji_candidate(inner_artist): artist_val = inner_artist
            if not group_val:
                 group_val = GLOBAL_GROUP_MAP.get(outer_group.lower())
                 if not group_val and is_romaji_candidate(outer_group) and outer_group: group_val = outer_group
        else:
            if not artist_val and content_lower in GLOBAL_ARTIST_MAP:
                artist_val = GLOBAL_ARTIST_MAP[content_lower]; continue
            if not group_val and content_lower in GLOBAL_GROUP_MAP:
                group_val = GLOBAL_GROUP_MAP[content_lower]; continue
            if is_romaji_candidate(content):
                if not artist_val: artist_val = content
                elif not group_val: group_val = content
    return artist_val, group_val

def is_folder_effectively_empty(folder_path: str) -> bool:
    try:
        return not any(entry.is_file() and entry.name.lower().endswith(('.zip', '.cbz', '.rar', '.cbr', '.7z', '.cb7', '.jpg', '.jpeg', '.png', '.gif', '.webp', '.bmp')) for entry in os.scandir(folder_path))
    except (PermissionError, FileNotFoundError): return False

def load_scan_cache(cache_path: str) -> dict:
    if not os.path.exists(cache_path): return {}
    try:
        with open(cache_path, 'r', encoding='utf-8') as f: return json.load(f)
    except Exception: return {}

def save_scan_cache(cache_path: str, data: dict):
    try:
        with open(cache_path, 'w', encoding='utf-8') as f: json.dump(data, f, indent=None, separators=(',', ':'))
    except IOError: pass

def handle_empty_folders(root_dir: str, quarantine_path: str, cache_path: str) -> set:
    if not quarantine_path: log_warning("[EH å¤–æ›] æœªè¨­å®šéš”é›¢å€è·¯å¾‘ï¼Œå°‡è·³éç©ºè³‡æ–™å¤¾è™•ç†ã€‚"); return set()
    
    log_info("[EH å¤–æ›] é–‹å§‹åŸ·è¡Œç©ºè³‡æ–™å¤¾éæ¿¾...")
    cache = load_scan_cache(cache_path); new_cache = {}; moved_folders = set(); summary.cache_misses = summary.cache_hits = 0
    try: all_local_folders = {entry.path: entry.stat().st_mtime for entry in os.scandir(root_dir) if entry.is_dir()}
    except FileNotFoundError: return moved_folders
    
    if not os.path.exists(quarantine_path): os.makedirs(quarantine_path)
    
    for folder_path, current_mtime in tqdm(all_local_folders.items(), desc="[EH å¤–æ›] éæ¿¾ç©ºè³‡æ–™å¤¾"):
        cache_entry = cache.get(folder_path)
        if cache_entry and cache_entry.get('mtime') == current_mtime: 
            is_empty = cache_entry.get('is_empty', False); summary.cache_hits += 1
        else: 
            is_empty = is_folder_effectively_empty(folder_path); summary.cache_misses += 1
        new_cache[folder_path] = {'mtime': current_mtime, 'is_empty': is_empty}
        
        if is_empty:
            try: 
                shutil.move(folder_path, os.path.join(quarantine_path, os.path.basename(folder_path)))
                moved_folders.add(normalize_path(folder_path))
            except Exception as e: log_warning(f"  -> ç§»å‹•ç©ºè³‡æ–™å¤¾å¤±æ•—: {folder_path} ({e})")

    save_scan_cache(cache_path, new_cache)
    summary.moved_empty = len(moved_folders)
    if moved_folders: log_info(f"  -> {len(moved_folders)} å€‹ç©ºè³‡æ–™å¤¾å·²è¢«ç§»å‹•è‡³éš”é›¢å€ã€‚")
    return moved_folders

def create_manga_record(folder_path, url_map):
    from nanoid import generate
    title = os.path.basename(folder_path)
    url = url_map.get(sanitize_filename(title), ""); normalized_fp = normalize_path(folder_path)
    sha1_hash = hashlib.sha1(normalized_fp.encode('utf-8')).hexdigest()
    mtime = os.path.getmtime(folder_path)
    return {"id": generate(), "title": title, "hash": sha1_hash, "filepath": os.path.normpath(folder_path), "filepath_normalized": normalized_fp, "type": "folder", "mtime": datetime.datetime.utcfromtimestamp(mtime).isoformat(timespec='milliseconds') + 'Z', "date": int(mtime * 1000), "status": "non-tag", "url": url, "tags": "{}", "rating": 0.0, "exist": 1, "createdAt": datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S'), "updatedAt": datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')}

def update_database_records(db_path, records_to_add=[], paths_to_soft_delete=[], paths_to_restore=[]):
    if not any([records_to_add, paths_to_soft_delete, paths_to_restore]): return
    with sqlite3.connect(db_path) as conn:
        cursor = conn.cursor()
        cursor.execute("CREATE UNIQUE INDEX IF NOT EXISTS idx_filepath_normalized ON Mangas(filepath_normalized)")
        if records_to_add:
            cursor.executemany("INSERT OR IGNORE INTO Mangas (id, title, hash, filepath, filepath_normalized, type, mtime, date, status, url, tags, rating, exist, createdAt, updatedAt) VALUES (:id, :title, :hash, :filepath, :filepath_normalized, :type, :mtime, :date, :status, :url, :tags, :rating, :exist, :createdAt, :updatedAt)", records_to_add)
            summary.added += cursor.rowcount
        if paths_to_soft_delete:
            cursor.executemany("UPDATE Mangas SET status = ?, updatedAt = datetime('now') WHERE filepath_normalized = ?", [('æª”æ¡ˆå·²è¢«åˆªé™¤', path) for path in paths_to_soft_delete])
            summary.soft_deleted += cursor.rowcount
        if paths_to_restore:
            cursor.executemany("UPDATE Mangas SET status = ?, updatedAt = datetime('now') WHERE filepath_normalized = ?", [('non-tag', path) for path in paths_to_restore])
            summary.restored += cursor.rowcount

def export_tag_failed_to_csv(config: Dict):
    log_info("[EH å¤–æ›] é–‹å§‹åŒ¯å‡º 'tag-failed' é …ç›®è‡³ CSV...")
    
    db_path = os.path.join(config.get('eh_data_directory'), "database.sqlite")
    if not os.path.exists(db_path):
        log_warning("[EH å¤–æ›] æ‰¾ä¸åˆ°è³‡æ–™åº«ï¼Œç„¡æ³•åŒ¯å‡º 'tag-failed' é …ç›®ã€‚"); return

    # === v-MOD START: å„ªå…ˆä½¿ç”¨è¨­å®šä¸­çš„è·¯å¾‘ ===
    output_csv_path = config.get('eh_csv_path')
    if not output_csv_path:
        # ä¿åº•ï¼šå¦‚æœè¨­å®šæª”æ²’å€¼ï¼Œæ‰ç”¨ data/tagfailed.csv (ç†è«–ä¸Š plugin_gui æœƒå¡«å…¥é è¨­å€¼)
        output_csv_path = os.path.join(DATA_DIR, 'tagfailed.csv')
    # === v-MOD END ===

    try:
        with sqlite3.connect(db_path) as conn:
            cursor = conn.execute("SELECT title, filepath, url FROM Mangas WHERE status = 'tag-failed'")
            failed_records = cursor.fetchall()

        if not failed_records:
            log_info("[EH å¤–æ›] è³‡æ–™åº«ä¸­æ²’æœ‰ 'tag-failed' çš„é …ç›®ï¼Œç„¡éœ€ç”Ÿæˆ CSVã€‚"); return

        csv_data = [['Title', 'Filepath', 'URL', 'Artist (Romaji)', 'Group (Romaji)']]
        for title, filepath, url in failed_records:
            artist_romaji, group_romaji = analyze_title_tags(title)
            csv_data.append([
                title or '',
                filepath or '',
                url or '',
                artist_romaji or '',
                group_romaji or '',
            ])
        
        if _atomic_write_csv_rows(csv_data, output_csv_path):
            log_info(f"[EH å¤–æ›] æˆåŠŸå°‡ {len(failed_records)} ç­† 'tag-failed' è¨˜éŒ„åŒ¯å‡ºè‡³: {output_csv_path}")
        else:
            log_error(f"[EH å¤–æ›] ç„¡æ³•å¯«å…¥ 'tag-failed' CSV æª”æ¡ˆï¼Œå¯èƒ½æª”æ¡ˆè¢«é–å®š: {output_csv_path}")

    except sqlite3.Error as e:
        log_error(f"[EH å¤–æ›] è®€å–è³‡æ–™åº«ä»¥åŒ¯å‡º 'tag-failed' é …ç›®æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
    except Exception as e:
        log_error(f"[EH å¤–æ›] åŒ¯å‡º 'tag-failed' CSV æ™‚ç™¼ç”ŸæœªçŸ¥éŒ¯èª¤: {e}", include_traceback=True)
        
        
def run_full_sync_headless(config: Dict, progress_queue: Optional[any]):
    _update_progress = lambda text, value=None: progress_queue.put({'type': 'progress' if value is not None else 'text', 'text': text, 'value': value}) if progress_queue else None
    log_info("[EH å¤–æ›] é–‹å§‹åŸ·è¡Œè³‡æ–™åº«å®Œæ•´åŒæ­¥...")
    
    root_dir = config.get('root_scan_folder')
    data_dir = config.get('eh_data_directory')
    db_path = os.path.join(data_dir, "database.sqlite")
    
    download_list_json_path = config.get('eh_mmd_json_path')
    url_map, json_data = {}, []
    if download_list_json_path and os.path.isfile(download_list_json_path):
        try:
            with open(download_list_json_path, 'r', encoding='utf-8') as f: json_data = json.load(f)
            url_map = {sanitize_filename(item['Name']): item['Url'] for item in json_data if item.get('Command') == 'Completed' and 'exhentai.org/g/' in item.get('Url', '') and item.get('Name')}
            log_info(f"[EH å¤–æ›] æˆåŠŸå¾ MMD JSON è¼‰å…¥ {len(url_map)} å€‹ URL æ˜ å°„ã€‚")
            update_csv_dashboard(json_data, config.get('eh_csv_path'))
        except Exception as e: log_error(f"[EH å¤–æ›] è®€å–æˆ–è§£æ MMD JSON æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
    else: log_warning("[EH å¤–æ›] æœªè¨­å®šæˆ–æ‰¾ä¸åˆ° MMD JSON æª”æ¡ˆï¼Œç„¡æ³•åŒ¹é… URL æˆ–æ›´æ–° CSVã€‚")
        
    quarantine_path = config.get('eh_quarantine_path')

    cache_path = os.path.join(DATA_DIR, 'scan_cache.json')
    log_info(f"[EH å¤–æ›] æƒæå¿«å–è·¯å¾‘å·²å®šä½è‡³: {cache_path}")

    moved_empty_folders = handle_empty_folders(root_dir, quarantine_path, cache_path)

    _update_progress("æ­£åœ¨æƒææœ¬åœ°è³‡æ–™å¤¾...", 20)
    try:
        local_paths = {normalize_path(entry.path) for entry in os.scandir(root_dir) if entry.is_dir()}
    except FileNotFoundError:
        log_error(f"[EH å¤–æ›] éŒ¯èª¤ï¼šæ‰¾ä¸åˆ°æŒ‡å®šçš„æ ¹ç›®éŒ„ '{root_dir}'"); return

    _update_progress("æ­£åœ¨è®€å–è³‡æ–™åº«è¨˜éŒ„...", 30)
    try:
        with sqlite3.connect(db_path) as conn: db_records = {row[0]: row[1] for row in conn.execute("SELECT filepath_normalized, status FROM Mangas")}
    except sqlite3.Error as e:
        log_error(f"[EH å¤–æ›] è®€å–è³‡æ–™åº«æ™‚ç™¼ç”Ÿåš´é‡éŒ¯èª¤: {e}"); return

    db_paths = set(db_records.keys())
    paths_to_add = local_paths - db_paths
    paths_to_soft_delete = {p for p in (db_paths - local_paths) if db_records.get(p) != 'æª”æ¡ˆå·²è¢«åˆªé™¤'}.union(moved_empty_folders)
    paths_to_restore = {p for p in (local_paths & db_paths) if db_records.get(p) == 'æª”æ¡ˆå·²è¢«åˆªé™¤'}

    log_info(f"[EH å¤–æ›] æ¯”å°å®Œæˆï¼š{len(paths_to_add)} å¾…æ–°å¢, {len(paths_to_soft_delete)} å¾…è»Ÿåˆªé™¤, {len(paths_to_restore)} å¾…é‚„åŸã€‚")
    
    new_records = [rec for path in tqdm(paths_to_add, desc="[EH å¤–æ›] è™•ç†æ–°è³‡æ–™å¤¾") if (rec := create_manga_record(path.replace('/', '\\'), url_map))] if paths_to_add else []
        
    update_database_records(db_path, records_to_add=new_records, paths_to_soft_delete=list(paths_to_soft_delete), paths_to_restore=list(paths_to_restore))
    _update_progress("è³‡æ–™åº«åŒæ­¥å®Œæˆã€‚", 50)
    log_info("[EH å¤–æ›] è³‡æ–™åº«å®Œæ•´åŒæ­¥å®Œæˆã€‚")

_PENDING_FILENAME = "download_dashboard_pending.jsonl"
_MAX_WRITE_RETRIES = 5
_WRITE_BACKOFF = 0.6

def _plugin_dir() -> Path:
    return Path(os.path.dirname(__file__))

def _pending_path() -> Path:
    return _plugin_dir() / _PENDING_FILENAME

def _atomic_write_text_to_path(path: Path, text: str, max_retries: int = _MAX_WRITE_RETRIES, backoff: float = _WRITE_BACKOFF) -> bool:
    path.parent.mkdir(parents=True, exist_ok=True)
    attempt = 0
    while attempt < max_retries:
        temp_name = None
        try:
            with tempfile.NamedTemporaryFile("w", delete=False, dir=str(path.parent), encoding="utf-8-sig", newline="") as tf:
                tf.write(text); temp_name = tf.name
            os.replace(temp_name, str(path))
            return True
        except PermissionError:
            if temp_name and os.path.exists(temp_name):
                try: os.remove(temp_name)
                except Exception: pass
            attempt += 1; time.sleep(backoff)
        except Exception:
            if temp_name and os.path.exists(temp_name):
                try: os.remove(temp_name)
                except Exception: pass
            raise
    return False

def _csv_rows_to_text(rows: List[List[Any]]) -> str:
    buf = io.StringIO(); writer = csv.writer(buf, lineterminator="\n")
    for r in rows: writer.writerow(r)
    return buf.getvalue()

def _append_pending_rows(rows: Iterable[Sequence[Any]]):
    p = _pending_path(); p.parent.mkdir(parents=True, exist_ok=True)
    with p.open("a", encoding="utf-8") as f:
        for row in rows:
            obj = {"Name": row[0] if len(row)>0 else "", "Url": row[1] if len(row)>1 else "", "Status": row[2] if len(row)>2 else "", "Artist": row[3] if len(row)>3 else "", "Group": row[4] if len(row)>4 else ""}
            f.write(json.dumps(obj, ensure_ascii=False) + "\n")

def _read_pending_items() -> List[Dict[str, Any]]:
    p = _pending_path()
    if not p.exists(): return []
    items: List[Dict[str, Any]] = []
    try:
        with p.open("r", encoding="utf-8") as f:
            for line in f:
                line = line.strip()
                if not line: continue
                try: items.append(json.loads(line))
                except Exception: continue
    except Exception: return []
    return items

def _clear_pending():
    p = _pending_path()
    try:
        if p.exists(): p.unlink()
    except Exception: pass

def _merge_pending_into_rows(all_rows: List[List[Any]], pending_items: Iterable[Dict[str, Any]]) -> bool:
    if not all_rows: all_rows.append(['Name','URL','Status','Artist (Romaji)','Group (Romaji)'])
    header = all_rows[0]; url_to_idx: Dict[str,int] = {}
    for i, row in enumerate(all_rows[1:], start=1):
        if len(row) > 1: url_to_idx[row[1]] = i
    changed = False
    for item in pending_items:
        url = item.get("Url") or item.get("URL") or item.get("url")
        if not url: continue
        name, status, artist, group = item.get("Name",""), item.get("Status",""), item.get("Artist",""), item.get("Group","")
        new_row = [name, url, status, artist, group]
        if url in url_to_idx:
            idx = url_to_idx[url]
            if all_rows[idx] != new_row:
                all_rows[idx] = new_row; changed = True
        else:
            all_rows.append(new_row); url_to_idx[url] = len(all_rows) - 1; changed = True
    return changed

def _atomic_write_csv_rows(all_rows: List[List[Any]], csv_path: str) -> bool:
    text = _csv_rows_to_text(all_rows)
    return _atomic_write_text_to_path(Path(csv_path), text)

def flush_pending_to_main(csv_path: str) -> bool:
    pending = _read_pending_items()
    if not pending: return True
    p = Path(csv_path)
    if p.exists():
        try:
            with p.open("r", encoding="utf-8-sig", newline="") as f:
                reader = csv.reader(f); rows = list(reader)
                if not rows: rows = [['Name','URL','Status','Artist (Romaji)','Group (Romaji)']]
        except Exception: return False
    else: rows = [['Name','URL','Status','Artist (Romaji)','Group (Romaji)']]
    changed = _merge_pending_into_rows(rows, pending)
    if not changed: _clear_pending(); return True
    ok = _atomic_write_csv_rows(rows, csv_path)
    if ok: _clear_pending(); return True
    else: return False

def update_csv_dashboard(json_data: list, csv_path: str):
    if not csv_path: log_warning("[EH å¤–æ›] æœªè¨­å®š CSV å„€è¡¨æ¿è·¯å¾‘ï¼Œè·³éæ›´æ–°ã€‚"); return
    try:
        if flush_pending_to_main(csv_path): log_info("[EH å¤–æ›] å·²å˜—è©¦åˆä½µå…ˆå‰çš„ pending è‡³ä¸» CSVã€‚")
    except Exception as e: log_warning(f"[EH å¤–æ›] åˆä½µ pending ç™¼ç”Ÿä¾‹å¤–ï¼š{e}")
    header = ['Name','URL','Status','Artist (Romaji)','Group (Romaji)']; rows: List[List[Any]] = []
    p = Path(csv_path)
    if p.exists():
        try:
            with p.open('r', encoding='utf-8-sig', newline='') as f:
                reader = csv.reader(f); rows = list(reader)
                if not rows or rows[0] != header: rows = [header]
        except Exception: rows = [header]
    else: rows = [header]
    url_to_idx: Dict[str,int] = {}
    for i, r in enumerate(rows[1:], start=1):
        if len(r) > 1: url_to_idx[r[1]] = i
    changed_rows: List[List[Any]] = []
    for it in json_data:
        url = it.get('Url')
        if not url: continue
        name, status = it.get('Name',''), it.get('Command','')
        artist_romaji, group_romaji = analyze_title_tags(name)
        new_row = [name, url, status, artist_romaji, group_romaji]
        if url in url_to_idx:
            idx = url_to_idx[url]
            if rows[idx] != new_row: rows[idx] = new_row; changed_rows.append(new_row)
        else: rows.append(new_row); url_to_idx[url] = len(rows) - 1; changed_rows.append(new_row)
    if not changed_rows: log_info("[EH å¤–æ›] CSV å„€è¡¨æ¿ç„¡è®Šæ›´ã€‚"); return
    if _atomic_write_csv_rows(rows, csv_path): log_info(f"[EH å¤–æ›] CSV å„€è¡¨æ¿æ›´æ–°å®Œæˆï¼š{csv_path}ï¼ˆå¯«å…¥ {len(changed_rows)} ç­†è®Šæ›´ï¼‰")
    else: _append_pending_rows(changed_rows); log_warning(f"[EH å¤–æ›] CSV è¢«é–å®šï¼Œå·²å°‡ {len(changed_rows)} ç­†è®Šæ›´å¯«å…¥ pendingï¼Œå¾…ä¸‹æ¬¡è‡ªå‹•åˆä½µã€‚")

def get_image_path(image_name: str) -> str:
    plugin_assets = os.path.join(os.path.dirname(__file__), 'assets', image_name)
    if os.path.exists(plugin_assets): return plugin_assets
    project_root = os.path.dirname(os.path.dirname(os.path.dirname(__file__)))
    return os.path.join(project_root, 'assets', image_name)

try:
    import numpy as np, cv2
    _HAS_CV2 = True
except Exception: _HAS_CV2 = False

_DEFAULT_CONFIDENCE = 0.85; _DEFAULT_TIMEOUT = 3.0; _SCALE_SET = [1.25, 1.10, 1.00, 0.90, 0.80]

def _pil_open_strict(path: str) -> Image.Image | None:
    try: return Image.open(path).convert('RGB')
    except Exception: return None

def _cv2_read_unicode(path: str):
    if not _HAS_CV2: return None
    try:
        data = np.fromfile(path, dtype=np.uint8); img = cv2.imdecode(data, cv2.IMREAD_COLOR)
        return img
    except Exception: return None

def _to_cv(bgr_or_pil):
    if not _HAS_CV2: return None
    if isinstance(bgr_or_pil, Image.Image):
        rgb = np.array(bgr_or_pil); return cv2.cvtColor(rgb, cv2.COLOR_RGB2BGR)
    return bgr_or_pil

def _match_template_cv(screen_bgr, needle_bgr, confidence: float):
    if not _HAS_CV2: return None
    H, W = needle_bgr.shape[:2]; best = None
    for scale in _SCALE_SET:
        try: resized = cv2.resize(needle_bgr, (int(W*scale), int(H*scale)), interpolation=cv2.INTER_AREA)
        except Exception: continue
        for use_gray in (False, True):
            src = cv2.cvtColor(screen_bgr, cv2.COLOR_BGR2GRAY) if use_gray else screen_bgr
            tpl = cv2.cvtColor(resized, cv2.COLOR_BGR2GRAY) if use_gray else resized
            res = cv2.matchTemplate(src, tpl, cv2.TM_CCOEFF_NORMED)
            min_val, max_val, min_loc, max_loc = cv2.minMaxLoc(res)
            if max_val >= confidence:
                h, w = tpl.shape[:2]; return (max_loc[0] + w//2, max_loc[1] + h//2)
            if (best is None) or (max_val > best[0]): best = (max_val, max_loc, tpl.shape[1], tpl.shape[0])
    return None

def _pillow_exact_match(screen_img: Image.Image, needle_img: Image.Image):
    try:
        import pyscreeze; pyscreeze.useOpenCV = False
        loc = pyautogui.locateCenterOnScreen(needle_img)
        return (loc.x, loc.y) if loc else None
    except Exception: return None

class _ScreenFinder:
    def __init__(self):
        try:
            import pyscreeze; pyscreeze.useOpenCV = bool(_HAS_CV2)
        except Exception: pass
    def _screenshot_cv(self):
        if not _HAS_CV2: return None
        return _to_cv(pyautogui.screenshot())
    def locate(self, image_name: str, confidence: float = _DEFAULT_CONFIDENCE, timeout: float = _DEFAULT_TIMEOUT):
        path = get_image_path(image_name)
        if not os.path.exists(path): log_error(f"[EH è‡ªå‹•åŒ–] æ‰¾ä¸åˆ°åœ–ç‰‡è³‡ç”¢: {path}"); return None
        needle_pil = _pil_open_strict(path)
        if needle_pil is None: log_error(f"[EH è‡ªå‹•åŒ–] åœ–ç‰‡æ ¼å¼ä¸æ”¯æ´æˆ–å·²æå£: {path}"); return None
        start = time.time()
        while time.time() - start < timeout:
            try:
                loc = pyautogui.locateCenterOnScreen(needle_pil, confidence=confidence) if _HAS_CV2 else pyautogui.locateCenterOnScreen(needle_pil)
                if loc: return (loc.x, loc.y)
            except Exception: break
            time.sleep(0.25)
            if _HAS_CV2:
                screen_bgr = self._screenshot_cv(); needle_bgr = _to_cv(needle_pil)
                if screen_bgr is not None and needle_bgr is not None:
                    if pt := _match_template_cv(screen_bgr, needle_bgr, confidence): return pt
            if pt := _pillow_exact_match(pyautogui.screenshot(), needle_pil): return pt
        return None
    def click(self, image_name: str, confidence: float = _DEFAULT_CONFIDENCE, timeout: float = _DEFAULT_TIMEOUT, delay: float = 0.4):
        if pt := self.locate(image_name, confidence=confidence, timeout=timeout):
            try: pyautogui.click(pt[0], pt[1]); time.sleep(delay); return True
            except Exception as e: log_warning(f"[EH è‡ªå‹•åŒ–] click å¤±æ•—: {e}")
        return False

SCREEN = _ScreenFinder()

def find_element(image_name: str, confidence: float = _DEFAULT_CONFIDENCE, timeout: float = _DEFAULT_TIMEOUT):
    if pt := SCREEN.locate(image_name, confidence=confidence, timeout=timeout):
        class _P: pass
        o = _P(); o.x, o.y = pt[0], pt[1]
        return o
    return None

def find_and_click(image_name: str, confidence: float = _DEFAULT_CONFIDENCE, timeout: float = _DEFAULT_TIMEOUT) -> bool:
    return SCREEN.click(image_name, confidence=confidence, timeout=timeout, delay=0.5)

def activate_window_by_pid(pid: int) -> bool:
    if not AUTOMATION_LIBS_AVAILABLE: return False
    found_hwnd = None
    def foreach_window(hwnd, lParam):
        nonlocal found_hwnd
        if ctypes.windll.user32.IsWindowVisible(hwnd):
            lpdwProcessId = ctypes.c_ulong()
            ctypes.windll.user32.GetWindowThreadProcessId(hwnd, ctypes.byref(lpdwProcessId))
            if lpdwProcessId.value == pid: found_hwnd = hwnd; return False
        return True
    try:
        EnumWindows = ctypes.windll.user32.EnumWindows
        WINFUNCTYPE = ctypes.WINFUNCTYPE(ctypes.c_bool, ctypes.POINTER(ctypes.c_int), ctypes.POINTER(ctypes.c_int))
        EnumWindows(WINFUNCTYPE(foreach_window), 0)
        if found_hwnd:
            ctypes.windll.user32.ShowWindow(found_hwnd, 3)
            ctypes.windll.user32.SetForegroundWindow(found_hwnd)
            log_info(f"  -> å·²æˆåŠŸæœ€å¤§åŒ–ä¸¦æ¿€æ´» PID ç‚º {pid} çš„è¦–çª—ã€‚"); return True
        else: log_warning(f"  -> æ‰¾ä¸åˆ° PID ç‚º {pid} çš„å¯è¦‹è¦–çª—ã€‚"); return False
    except Exception as e: log_error(f"  -> æ¿€æ´»è¦–çª—æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}"); return False

def _get_current_hkl():
    if not AUTOMATION_LIBS_AVAILABLE: return None
    try: return ctypes.windll.user32.GetKeyboardLayout(ctypes.windll.user32.GetWindowThreadProcessId(ctypes.windll.user32.GetForegroundWindow(), None))
    except Exception: return None

def ensure_english_input():
    if not AUTOMATION_LIBS_AVAILABLE: return
    try: ctypes.windll.user32.ActivateKeyboardLayout(ctypes.windll.user32.LoadKeyboardLayoutA(b"00000409", 1), 256)
    except Exception as e: log_warning(f"åˆ‡æ›è‡³è‹±æ–‡è¼¸å…¥æ³•å¤±æ•—: {e}")

def restore_keyboard_layout(original_hkl):
    if original_hkl and AUTOMATION_LIBS_AVAILABLE:
        try: ctypes.windll.user32.ActivateKeyboardLayout(original_hkl, 256)
        except Exception as e: log_warning(f"é‚„åŸè¼¸å…¥æ³•å¤±æ•—: {e}")

_LOG_DIR = os.path.join(os.path.dirname(__file__), "logs"); os.makedirs(_LOG_DIR, exist_ok=True)
_CHILD_LOG = os.path.join(_LOG_DIR, "eh_manager_child.log"); _FILTER_TAGS = ("EBUSY", "Saved", "unlink", "Error", "WARN", "scanned", "Digest")

def _spawn_eh_manager(app_path: str):
    try:
        p = subprocess.Popen([app_path], stdout=subprocess.PIPE, stderr=subprocess.STDOUT, text=True, encoding="utf-8", errors="replace", creationflags=subprocess.CREATE_NO_WINDOW)
    except Exception as e: log_error(f"[EH è‡ªå‹•åŒ–] ç„¡æ³•å•Ÿå‹•æ‡‰ç”¨ç¨‹å¼ï¼š{e}"); return None
    qlines = queue.Queue()
    def _reader():
        with open(_CHILD_LOG, "a", encoding="utf-8") as fout:
            for line in iter(p.stdout.readline, ""):
                fout.write(line); fout.flush()
                if any(tag in line for tag in _FILTER_TAGS): qlines.put(line.rstrip("\n"))
    t = threading.Thread(target=_reader, daemon=True); t.start()
    def _drain():
        while True:
            try: log_info(f"[EHM] {qlines.get(timeout=0.2)}")
            except queue.Empty:
                if p.poll() is not None and qlines.empty(): break
    threading.Thread(target=_drain, daemon=True).start()
    return p

def close_manga_app_if_running(config: Dict):
    """
    æª¢æŸ¥ä¸¦é—œé–‰ç›®æ¨™æ‡‰ç”¨ç¨‹å¼ã€‚
    v1.9.10: ä½¿ç”¨ try...except åŒ…è£¹ï¼Œç¢ºä¿å³ä½¿å‡ºéŒ¯ä¹Ÿä¸æœƒä¸­æ–·ä¸»ç¨‹å¼ã€‚
    """
    if not AUTOMATION_LIBS_AVAILABLE: return
    manga_app_path = config.get('eh_manga_manager_path', '')
    if not manga_app_path:
        log_warning("[EH è‡ªå‹•åŒ–] è¨­å®šä¸­æœªæä¾› manga_manager_pathï¼Œè·³éé—œé–‰ç¨‹åºã€‚"); return
    
    target_app_name = os.path.basename(manga_app_path)
    log_info(f"[EH è‡ªå‹•åŒ–] æª¢æŸ¥ '{target_app_name}' åŸ·è¡Œç‹€æ…‹...") 
    
    try:
        found_count = 0
        for proc in psutil.process_iter(['name', 'pid']):
            try:
                if proc.info['name'] and proc.info['name'].lower() == target_app_name.lower():
                    log_info(f"  -> ç™¼ç¾é€²ç¨‹ (PID: {proc.pid})ï¼Œæ­£åœ¨é—œé–‰...")
                    try:
                        proc.terminate()
                        proc.wait(timeout=3)
                    except (psutil.NoSuchProcess, psutil.TimeoutExpired):
                        proc.kill()
                        proc.wait(timeout=3)
                    found_count += 1
            except (psutil.NoSuchProcess, psutil.AccessDenied):
                continue
        
        if found_count > 0:
            log_info(f"  -> å·²é—œé–‰ {found_count} å€‹å¯¦ä¾‹ã€‚")
        else:
            log_info(f"  -> æœªç™¼ç¾é‹è¡Œä¸­çš„æ‡‰ç”¨ç¨‹å¼ï¼Œç„¡éœ€æ“ä½œã€‚")
            
    except Exception as e:
        log_warning(f"[EH è‡ªå‹•åŒ–] æª¢æŸ¥é€²ç¨‹æ™‚ç™¼ç”Ÿç•°å¸¸ (å·²å¿½ç•¥): {e}")
        # é€™è£¡ä¸æ‹‹å‡ºç•°å¸¸ï¼Œç¢ºä¿ä¸»ç¨‹å¼å¯ä»¥ç¹¼çºŒåŸ·è¡Œ

def count_untagged_manga(db_path: str) -> int:
    if not os.path.exists(db_path): return 0
    try:
        with sqlite3.connect(db_path) as conn:
            conn.execute("PRAGMA query_only = ON")
            return conn.execute("SELECT COUNT(*) FROM Mangas WHERE status = 'non-tag'").fetchone()[0]
    except sqlite3.Error: return 0

def run_automation_suite_headless(config: Dict, progress_queue: Optional[any], control_events: Dict):
    if not AUTOMATION_LIBS_AVAILABLE: log_error("[EH å¤–æ›] ç¼ºå°‘ UI è‡ªå‹•åŒ–å‡½å¼åº«ï¼Œç„¡æ³•åŸ·è¡Œå…ƒæ•¸æ“šæ›´æ–°ã€‚"); return
    _update_progress = lambda text, value=None: progress_queue.put({'type': 'progress' if value is not None else 'text', 'text': text, 'value': value}) if progress_queue else None
    timing = _init_automation_speed_from_config(config)
    CLICK_DELAY, PAGEDOWN_DELAY, AFTER_SCROLL = timing["CLICK"], timing["PAGEDOWN"], timing["AFTER_SCROLL"]

    log_info("[EH å¤–æ›] UI è‡ªå‹•åŒ–æµç¨‹é–‹å§‹...")
    db_path = os.path.join(config.get('eh_data_directory'), "database.sqlite")
    task_limit = count_untagged_manga(db_path)
    summary.tasks_total = task_limit
    
    if task_limit == 0:
        log_info("[EH å¤–æ›] è³‡æ–™åº«ä¸­æ²’æœ‰ non-tag é …ç›®ï¼Œç„¡éœ€åŸ·è¡Œ UI è‡ªå‹•åŒ–ã€‚"); _update_progress("è³‡æ–™åº«ç„¡éœ€æ›´æ–°ã€‚", 100); return

    _update_progress(f"æª¢æ¸¬åˆ° {task_limit} å€‹é …ç›®éœ€è¦æ›´æ–°å…ƒæ•¸æ“š...", 55)
    
    app_path = config.get("eh_manga_manager_path")
    proc = _spawn_eh_manager(app_path)
    if not proc: _update_progress("âŒ ç¨‹å¼å•Ÿå‹•å¤±æ•—ã€‚", 100); return
    app_pid = proc.pid
    time.sleep(float(config.get('automation_page_load_delay', 2.0)) * 3)

    if not activate_window_by_pid(app_pid):
        log_error("[EH è‡ªå‹•åŒ–] è¦–çª—æ¿€æ´»å¤±æ•—ï¼Œè‡ªå‹•åŒ–ä¸­æ­¢ã€‚"); _update_progress("âŒ éŒ¯èª¤: ç¨‹å¼è¦–çª—æ¿€æ´»å¤±æ•—ã€‚", 100); return

    original_hkl = None
    try:
        original_hkl = _get_current_hkl()
        _update_progress("æ­£åœ¨å®šä½ UI å…ƒç´ ...", 60)
        search_icon = find_element(MAIN_SEARCH_ICON_IMG, timeout=10)
        if not search_icon: log_error("[EH è‡ªå‹•åŒ–] æ‰¾ä¸åˆ°ä¸»æœå°‹æ¡†éŒ¨é»ï¼"); _update_progress("âŒ éŒ¯èª¤: æ‰¾ä¸åˆ°ä¸»æœå°‹æ¡†ã€‚"); return
        
        pyautogui.click(search_icon.x + SEARCH_BOX_X_OFFSET, search_icon.y)
        ensure_english_input(); pyperclip.copy('"non-tag"$'); pyautogui.hotkey('ctrl', 'v'); pyautogui.press('enter')

        log_info("[EH è‡ªå‹•åŒ–] å·²åŸ·è¡Œæœå°‹ï¼Œæ­£åœ¨ä¸»å‹•è¼ªè©¢ç­‰å¾… UI çµæœå‡ºç¾...")
        first_target = None; wait_start_time = time.time()
        while time.time() - wait_start_time < 15:
            first_target = find_element(BOOKMARK_ICON_IMG, timeout=0.5) or find_element(BOOKMARK_ICON_READY_IMG, timeout=0.5)
            if first_target: log_info(f"[EH è‡ªå‹•åŒ–] ç›®æ¨™å·²å‡ºç¾ï¼(è€—æ™‚ {time.time() - wait_start_time:.2f} ç§’)"); break
            time.sleep(0.5)

        if not first_target: log_warning("[EH è‡ªå‹•åŒ–] ç­‰å¾…è¶…æ™‚ (15ç§’)ï¼Œä»æœªåœ¨è¢å¹•ä¸Šæ‰¾åˆ°ä»»ä½• non-tag é …ç›®ï¼Œæµç¨‹çµæŸã€‚"); find_and_click(CLEAR_SEARCH_BUTTON_IMG); return

        _update_progress("æ­£åœ¨é–‹å§‹è‡ªå‹•åŒ–è¿´åœˆ...", 65)
        pyautogui.click(first_target.x + TITLE_X_OFFSET, first_target.y + TITLE_Y_OFFSET)
        time.sleep(PAGE_LOAD_DELAY)
        
        for i in range(task_limit):
            if control_events['cancel'].is_set(): log_info("[EH è‡ªå‹•åŒ–] æ”¶åˆ°å–æ¶ˆè¨Šè™Ÿï¼Œæµç¨‹çµ‚æ­¢ã€‚"); break
            while control_events['pause'].is_set(): time.sleep(0.2)
            
            summary.tasks_processed = i + 1
            progress_val = 65 + int(30 * (summary.tasks_processed / task_limit))
            _update_progress(f"æ­£åœ¨è™•ç†ç¬¬ {summary.tasks_processed}/{task_limit} æœ¬...", progress_val)
            
            if find_and_click(RESCAN_BUTTON_IMG, timeout=5): time.sleep(CLICK_DELAY)
            if summary.tasks_processed >= task_limit: break
            
            pyautogui.press('pagedown'); time.sleep(PAGEDOWN_DELAY); time.sleep(AFTER_SCROLL)
            
            if find_element(PAGE_END_IMG, timeout=1): log_info("[EH è‡ªå‹•åŒ–] åµæ¸¬åˆ°é é¢æœ«ç«¯ï¼Œæå‰çµæŸã€‚"); break
                
        if find_element(CLOSE_BUTTON_IMG, timeout=1): find_and_click(CLOSE_BUTTON_IMG, timeout=2)
        find_and_click(CLEAR_SEARCH_BUTTON_IMG, timeout=5)

    except Exception as e: log_error(f"[EH è‡ªå‹•åŒ–] è‡ªå‹•åŒ–éç¨‹ä¸­ç™¼ç”ŸéŒ¯èª¤: {e}", include_traceback=True); _update_progress(f"âŒ è‡ªå‹•åŒ–éŒ¯èª¤: {e}")
    finally:
        if original_hkl: restore_keyboard_layout(original_hkl)

def create_database_backup(config: Dict):
    BACKUPS_TO_KEEP = 3; log_info("[EH å¤–æ›] æ­£åœ¨æª¢æŸ¥ä¸¦åŸ·è¡Œè³‡æ–™åº«å‚™ä»½...")
    backup_dir = config.get('eh_backup_directory')
    if not backup_dir: log_info("  -> æœªè¨­å®šå‚™ä»½è³‡æ–™å¤¾ï¼Œè·³éå‚™ä»½ç¨‹åºã€‚"); return
    data_dir = config.get('eh_data_directory')
    source_db_path = os.path.join(data_dir, "database.sqlite")
    if not os.path.exists(source_db_path): log_warning(f"  -> æ‰¾ä¸åˆ°ä¾†æºè³‡æ–™åº«æª”æ¡ˆï¼Œç„¡æ³•å‚™ä»½: {source_db_path}"); return
    try:
        os.makedirs(backup_dir, exist_ok=True)
        timestamp = datetime.datetime.now().strftime('%Y-%m-%d_%H%M%S')
        backup_filename = f"database_{timestamp}.sqlite"
        destination_path = os.path.join(backup_dir, backup_filename)
        shutil.copy2(source_db_path, destination_path)
        log_info(f"  -> è³‡æ–™åº«æˆåŠŸå‚™ä»½è‡³: {destination_path}")

        log_info(f"  -> æ­£åœ¨æ¸…ç†èˆŠå‚™ä»½ï¼Œåƒ…ä¿ç•™æœ€æ–°çš„ {BACKUPS_TO_KEEP} å€‹...")
        all_backups = sorted([f for f in os.listdir(backup_dir) if f.startswith('database_') and f.endswith('.sqlite')])
        if len(all_backups) > BACKUPS_TO_KEEP:
            to_delete = all_backups[:-BACKUPS_TO_KEEP]
            log_info(f"  -> ç™¼ç¾ {len(all_backups)} å€‹å‚™ä»½ï¼Œå°‡åˆªé™¤ {len(to_delete)} å€‹æœ€èˆŠçš„å‚™ä»½ã€‚")
            for old_backup in to_delete:
                try:
                    os.remove(os.path.join(backup_dir, old_backup))
                    log_info(f"    - å·²åˆªé™¤èˆŠå‚™ä»½: {old_backup}")
                except OSError as e: log_error(f"    - åˆªé™¤èˆŠå‚™ä»½ {old_backup} å¤±æ•—: {e}")
        else: log_info(f"  -> ç•¶å‰å‚™ä»½æ•¸é‡ ({len(all_backups)}) æœªè¶…éé™åˆ¶ï¼Œç„¡éœ€æ¸…ç†ã€‚")
    except Exception as e: log_error(f"[EH å¤–æ›] å»ºç«‹æˆ–æ¸…ç†è³‡æ–™åº«å‚™ä»½æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}", include_traceback=True)

class EhDatabaseToolsPlugin(BasePlugin):
    def get_id(self) -> str: return "eh_database_tools"
    def get_name(self) -> str: return "exhentai-manga-manager è³‡æ–™åº«æ›´æ–°å·¥å…·"
    def get_description(self) -> str: return "åœ¨æ¯æ¬¡æƒæå‰ï¼Œè‡ªå‹•åŒæ­¥ EH è³‡æ–™åº«ã€æ›´æ–° CSV ä¸¦é€é UI è‡ªå‹•åŒ–æ›´æ–°å…ƒæ•¸æ“šã€‚"
    def get_plugin_type(self) -> str: return 'preprocessor'
    def get_default_config(self):
        return {"enable_eh_preprocessor": False, "eh_data_directory": "", "eh_backup_directory": "", "eh_syringe_directory": "", "eh_mmd_json_path": ""}
    def get_slot_order(self) -> int: return 10
    def plugin_prefers_inner_enable(self) -> bool: return True
    def get_settings_frame(self, parent_frame: 'ttk.Frame', config: Dict[str, Any], ui_vars: Dict) -> Optional['ttk.Frame']:
        from . import plugin_gui
        return plugin_gui.create_settings_frame(parent_frame, config, ui_vars)
    def save_settings(self, config: Dict[str, Any], ui_vars: Dict) -> Dict[str, Any]:
        from . import plugin_gui
        return plugin_gui.save_settings(config, ui_vars)

    def run(self, config: Dict, progress_queue: Optional[any], control_events: Optional[Dict], app_update_callback=None):
        global summary
        summary = ExecutionSummary(); summary.mode = "å‰ç½®è™•ç†"
        try: from nanoid import generate
        except ImportError:
            log_error("[EH å¤–æ›] ç¼ºå°‘å¿…è¦çš„å‡½å¼åº« 'nanoid'ã€‚è«‹åŸ·è¡Œ 'pip install nanoid'ã€‚")
            if progress_queue: progress_queue.put({'type':'text', 'text': "âŒ [EH å¤–æ›] éŒ¯èª¤: ç¼ºå°‘ nanoid å‡½å¼åº«ã€‚"})
            return
        _update_progress = lambda text, value=None: progress_queue.put({'type': 'progress' if value is not None else 'text', 'text': text, 'value': value}) if progress_queue else None
        create_database_backup(config)
        try:
            _update_progress("ğŸš€ [EH å‰ç½®è™•ç†] é–‹å§‹åŸ·è¡Œ...", 0)
            try: flush_pending_to_main(config.get("eh_csv_path", "download_dashboard.csv"))
            except Exception: pass
            required_paths = ['eh_data_directory', 'root_scan_folder']
            if config.get('automation_enabled', False): required_paths.append('eh_manga_manager_path')
            if not all(config.get(p) and os.path.exists(config.get(p)) for p in required_paths):
                log_error("[EH å¤–æ›] è¨­å®šä¸­çš„ä¸€å€‹æˆ–å¤šå€‹å¿…è¦è·¯å¾‘ç„¡æ•ˆæˆ–ä¸å­˜åœ¨ã€‚"); _update_progress("âŒ éŒ¯èª¤: å¤–æ›è·¯å¾‘è¨­å®šä¸å®Œæ•´æˆ–ç„¡æ•ˆã€‚"); return
            
            # --- å¼·åˆ¶ç¹¼çºŒé‚è¼¯ ---
            if config.get('automation_enabled', False):
                close_manga_app_if_running(config)
            # å¼·åˆ¶çµ¦äºˆå›é¥‹ï¼Œè¡¨æ˜æµç¨‹å·²æ¨é€²
            _update_progress("æ­£åœ¨é€£æ¥è³‡æ–™åº«...", 10) 
            # -------------------

            if control_events and control_events['cancel'].is_set(): return
            data_dir = config.get('eh_data_directory')
            db_path = os.path.join(data_dir, "database.sqlite")
            if not os.path.isfile(db_path): _update_progress("âŒ [EH å¤–æ›] éŒ¯èª¤: æ‰¾ä¸åˆ° database.sqliteã€‚"); return
            add_normalized_path_column_if_not_exists(db_path)
            migrate_to_v20_structure(db_path)
            if control_events and control_events['cancel'].is_set(): return
            load_translation_maps(config)
            if control_events and control_events['cancel'].is_set(): return
            run_full_sync_headless(config, progress_queue)
            if control_events and control_events['cancel'].is_set(): return
            db_path = os.path.join(config.get('eh_data_directory'), "database.sqlite")
            try:
                log_info("[EH å¤–æ›] æ­£åœ¨å¼·åˆ¶åŒæ­¥è³‡æ–™åº«æ—¥èªŒ (WAL Checkpoint)...")
                with sqlite3.connect(db_path) as conn: conn.execute("PRAGMA wal_checkpoint(TRUNCATE);")
                log_info("  -> è³‡æ–™åº«æ—¥èªŒåŒæ­¥å®Œæˆã€‚")
            except sqlite3.OperationalError as e:
                if "database is locked" in str(e):
                    log_warning("[EH å¤–æ›] è³‡æ–™åº«è¢«é–å®šï¼Œç­‰å¾… 1 ç§’å¾Œé‡è©¦ Checkpoint...")
                    time.sleep(1)
                    try:
                        with sqlite3.connect(db_path) as conn: conn.execute("PRAGMA wal_checkpoint(TRUNCATE);")
                        log_info("  -> é‡è©¦æˆåŠŸï¼")
                    except Exception as retry_e: log_error(f"[EH å¤–æ›] Checkpoint é‡è©¦å¤±æ•—: {retry_e}")
                else: log_error(f"[EH å¤–æ›] WAL Checkpoint åŸ·è¡Œå¤±æ•—: {e}")
            except Exception as e: log_error(f"[EH å¤–æ›] WAL Checkpoint ç™¼ç”ŸæœªçŸ¥éŒ¯èª¤: {e}")
            if config.get('automation_enabled', False):
                if AUTOMATION_LIBS_AVAILABLE: run_automation_suite_headless(config, progress_queue, control_events)
                else: log_warning("[EH å¤–æ›] è·³é UI è‡ªå‹•åŒ–ï¼Œç¼ºå°‘å¿…è¦å‡½å¼åº«(pyautogui/psutil ç­‰)ã€‚")
            else: log_info("[EH å¤–æ›] UI è‡ªå‹•åŒ–åŠŸèƒ½å·²åœ¨è¨­å®šä¸­è¢«ç¦ç”¨ï¼Œè·³éæ­¤æ­¥é©Ÿã€‚")
            _update_progress("âœ… [EH å‰ç½®è™•ç†] å®Œæˆï¼", 100)
        except Exception as e:
            log_error(f"[EH å¤–æ›] åŸ·è¡ŒæœŸé–“ç™¼ç”Ÿåš´é‡éŒ¯èª¤: {e}", include_traceback=True)
            if progress_queue: progress_queue.put({'type':'text', 'text': f"âŒ [EH å¤–æ›] éŒ¯èª¤: {e}"})
        finally:
            try: flush_pending_to_main(config.get("eh_csv_path", "download_dashboard.csv"))
            except Exception: pass
        try: export_tag_failed_to_csv(config)
        except Exception as e: log_error(f"[EH å¤–æ›] åŸ·è¡Œ tag-failed åŒ¯å‡ºæ™‚ç™¼ç”Ÿä¾‹å¤–: {e}")
        if summary: summary.report()